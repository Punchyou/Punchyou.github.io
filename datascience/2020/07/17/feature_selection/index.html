<!DOCTYPE html> <html lang=" en "><head> <meta charset="utf-8"> <title>Maria Pantsiou - Software Developer @ARM</title> <meta http-equip="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <meta name="description" content="Feature Selection" /> <meta name="keywords" content="Feature Selection, Maria Pantsiou, dataScience" /> <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml"> <meta content="" property="fb:app_id"> <meta content="Maria Pantsiou" property="og:site_name"> <meta content="Feature Selection" property="og:title"> <meta content="article" property="og:type"> <meta content="Feature Selection" property="og:description"> <meta content="/datascience/2020/07/17/feature_selection/" property="og:url"> <meta content="2020-07-17T09:05:23+00:00" property="article:published_time"> <meta content="/about/" property="article:author"> <meta content="dataScience" property="article:section"> <meta name="twitter:card" content="summary"> <meta name="twitter:site" content="@"> <meta name="twitter:creator" content="@"> <meta name="twitter:title" content="Feature Selection"> <meta content="Maria Pantsiou" property="og:site_name"> <meta name="twitter:url" content="/datascience/2020/07/17/feature_selection/"> <meta name="twitter:description" content="devlopr-jekyll is a Jekyll Theme Built For Developers"> <link rel="stylesheet" href="/assets/css/main.css" /> <link rel="stylesheet" href="/assets/css/custom-style.css" /> <link rel="stylesheet" href="/assets/bower_components/lightgallery/dist/css/lightgallery.min.css"/> <link rel="stylesheet" href="https://cdn.snipcart.com/themes/v3.0.0-beta.3/default/snipcart.css" /> <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch.min.css"> <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch-theme-algolia.min.css"> <link rel="stylesheet" href="/assets/bower_components/bootstrap/dist/css/bootstrap.min.css" /> <link rel="stylesheet" href="/assets/bower_components/font-awesome/web-fonts-with-css/css/fontawesome-all.min.css" /> <!-- Fonts--> <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet"> <!-- Favicon --> <link rel="icon" href="/assets/img/favicon.ico" type="image/gif" sizes="16x16"> <!-- Jquery --> <script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha384-vk5WoKIaW/vJyUAd9n/wmopsmNhiy+L2Z+SBxGYnUkunIxVxAv/UtMOhba/xskxh" crossorigin="anonymous"></script> <!-- <script src="/assets/bower_components/jquery/dist/jquery.min.js"></script> --> <script src="/assets/bower_components/jquery.easing/jquery.easing.min.js"></script> <script src="/assets/bower_components/bootstrap/dist/js/bootstrap.bundle.min.js"></script> <script src="/assets/bower_components/jquery-mousewheel/jquery.mousewheel.min.js"></script> <script src="/assets/bower_components/lightgallery/dist/js/lightgallery-all.min.js"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/picturefill/3.0.2/picturefill.min.js"></script> <script src="/assets/bower_components/imagesloaded/imagesloaded.pkgd.min.js"></script> <script src="/assets/bower_components/nanobar/nanobar.min.js"></script> <script src="/assets/bower_components/typewrite/dist/typewrite.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch.min.js"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.20.1/moment.min.js"></script> <!-- Github Button --> <script async defer src="https://buttons.github.io/buttons.js"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script> <script> (function (i, s, o, g, r, a, m) { i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () { (i[r].q = i[r].q || []).push(arguments) }, i[r].l = 1 * new Date(); a = s.createElement(o), m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m) })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga'); ga('create', '', 'auto'); ga('send', 'pageview'); </script> </head> <body> <div class="container-fluid"><header> <div class="col-lg-12"> <div class="row"> <nav class="navbar navbar-expand-lg fixed-top navbar-dark " id="topNav"> <!-- <a class="navbar-brand" href="#">Maria Pantsiou</a> --> <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button> <a class="navbar-brand" href="/">Maria Pantsiou</a> <div class="collapse navbar-collapse" id="navbarNav"> <ul class="navbar-nav"> <li class="nav-item"> <a class="nav-link" href="/about">About Me</a> </li> <li class="nav-item"> <a class="nav-link" href="/blog">Blog</a> </li> </ul> </div> <ul class="nav justify-content-end"> <li class="nav-item"> <a class="nav-link" id="search-icon" href="/search/"><i class="fa fa-search" aria-hidden="true"></i></a> </li> <li class="nav-item"> <input class="nav-link switch" id="theme-toggle" onclick="modeSwitcher() "type="checkbox" name="checkbox" > </li> </ul> </nav> </div> </div> </header><div class="col-lg-12"> <!-- Blog Post Breadcrumbs --><div class="col-lg-12"> <nav aria-label="breadcrumb" role="navigation"> <ol class="breadcrumb"> <li class="breadcrumb-item"> <a href="/blog"><i class="fa fa-home" aria-hidden="true"></i></a> </li> <li class="breadcrumb-item active" aria-current="page"><a href="/datascience/2020/07/17/feature_selection/">Feature Selection</a></li> </ol> </nav> </div> <div class="row" id="blog-post-container"> <div class="col-lg-8 offset-md-2"><article class="card" itemscope itemtype="http://schema.org/BlogPosting"> <div class="card-header"> <!-- <h1 class="post-title" itemprop="name headline">Feature Selection</h1> --> <h4 class="post-meta">Feature Selection</h4> <p class="post-summary">Posted by : <img src="/assets/img/avatar.png" class="author-profile-img"> <span itemprop="author" itemscope itemtype="http://schema.org/Person"> <span itemprop="name">Maria Pantsiou</span> </span> at <time datetime="2020-07-17 09:05:23 +0000" itemprop="datePublished">Jul 17, 2020</time> </p> <span class="disqus-comment-count" data-disqus-identifier="/datascience/2020/07/17/feature_selection/"></span> <div class="post-categories"> Category : <a href="/blog/categories/dataScience">dataScience</a> </div> </div> <div class="card-body" itemprop="articleBody"> <img class="card-img-top" src="/assets/img/posts/ico_sphere_bg.png" alt=""> <br/> <br/> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], displayMath: [['$$','$$']], inlineMath: [['$','$']], }, }); </script> <h1 id="feature-selection-algorithms">Feature Selection Algorithms</h1> <p><em>This article is about selecting the best set of features for your model. I did some research on this topic and ended up finding really interesting resources that refer to different algotihms that can be parts of a robust feature selection tool. Take a look on what I’ve found:</em></p> <h1 id="why-feature-selection">Why Feature Selection?</h1> <ol> <li><strong>When having too many features the model overfits due to the curse of dimentionality.</strong> If we have more columns in the data than the number of rows, we will be able to fit our training data perfectly, but that won’t generalize to the new samples. And thus we learn absolutely nothing</li> <li><strong>Occam’s Razor</strong>: We want our models to be simple and explainable, and having multiple features doesn’t help to that</li> <li><strong>Garbage In Garbage out</strong>: Poor-quality input will produce Poor-Quality output</li> </ol> <p>Four categories of feature selection process:</p> <ol> <li><strong>Filter Based</strong>: An example of such a metric could be correlation with the ouput (Gram-Schmidt, mutual information), PCA or chi-square</li> <li><strong>Wrapper-based</strong>: Wrapper methods consider the selection of a set of features as a search problem. Example: <em>Recursive Feature Elimination</em>. Disadvandages are the overfitting risk and computation time.</li> <li><strong>Embedded</strong>: Embedded methods use algorithms that have built-in feature selection methods. For instance, <em>Lasso</em> and <em>RF</em> have their own feature selection methods.</li> <li><strong>Hybrid</strong>: Combine filter (for dimentionality reduction) and wrappers (to find the best subset).</li> </ol> <h2 id="implementation">Implementation</h2> <p>The problem that we want to solve is to find the best features that can tell us if a football player is “great” or not, based on dataset of existing football players. Our training dataset consists of a column that contains names of famous football players to date, columns with different information about those players and an <em>Overall</em> column (range 46 - 94) based on which we will decide where a player is “great” or not. In our case, if the <em>Overall</em> score is more than 87, then we consider the player “great”.</p> <p><em>You can find the code of this article <a href="https://github.com/Punchyou/my_examples_and_notes/blob/master/feature_selection/feature_selection.py">here</a>, but you can also continue reading to see in more detail the steps I followed.</em></p> <p>Use the following code to import all the libraries we’ll use thoughout this article’s code and to prepare the example dataset for this implementation:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">scipy.stats</span> <span class="k">as</span> <span class="n">ss</span>
<span class="kn">import</span> <span class="nn">math</span> 
<span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="n">pd</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="n">sns</span>
<span class="kn">import</span> <span class="nn">re</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span> <span class="nn">collections</span> <span class="kn">import</span> <span class="n">Counter</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectKBest</span><span class="p">,</span> <span class="n">chi2</span>
<span class="kn">from</span> <span class="nn">sklearn.preprocessing</span> <span class="kn">import</span> <span class="n">MinMaxScaler</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">SelectFromModel</span><span class="p">,</span> <span class="n">RFE</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LogisticRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.ensemble</span> <span class="kn">import</span> <span class="n">RandomForestClassifier</span>
<span class="kn">from</span> <span class="nn">lightgbm</span> <span class="kn">import</span> <span class="n">LGBMClassifier</span>

<span class="n">sns</span><span class="o">.</span><span class="nb">set</span><span class="p">(</span><span class="n">style</span><span class="o">=</span><span class="s">"ticks"</span><span class="p">)</span> <span class="c1">#remove grid lines
</span><span class="n">flatui</span> <span class="o">=</span> <span class="p">[</span><span class="s">"#9b59b6"</span><span class="p">,</span> <span class="s">"#3498db"</span><span class="p">,</span> <span class="s">"#95a5a6"</span><span class="p">,</span> <span class="s">"#e74c3c"</span><span class="p">,</span> <span class="s">"#34495e"</span><span class="p">,</span> <span class="s">"#2ecc71"</span><span class="p">]</span>
<span class="n">flatui</span> <span class="o">=</span> <span class="n">sns</span><span class="o">.</span><span class="n">color_palette</span><span class="p">(</span><span class="n">flatui</span><span class="p">)</span>

<span class="c1">#upload dataset
</span><span class="n">player_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="s">"https://raw.githubusercontent.com/amanthedorkknight/fifa18-all-player-statistics/master/2019/data.csv"</span><span class="p">)</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="s">"Unnamed: 0"</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

<span class="c1"># make categorical into numberical
</span><span class="n">numerical_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Overall'</span><span class="p">,</span> <span class="s">'Crossing'</span><span class="p">,</span><span class="s">'Finishing'</span><span class="p">,</span>  <span class="s">'ShortPassing'</span><span class="p">,</span>  <span class="s">'Dribbling'</span><span class="p">,</span><span class="s">'LongPassing'</span><span class="p">,</span> <span class="s">'BallControl'</span><span class="p">,</span> <span class="s">'Acceleration'</span><span class="p">,</span><span class="s">'SprintSpeed'</span><span class="p">,</span> <span class="s">'Agility'</span><span class="p">,</span>  <span class="s">'Stamina'</span><span class="p">,</span><span class="s">'Volleys'</span><span class="p">,</span><span class="s">'FKAccuracy'</span><span class="p">,</span><span class="s">'Reactions'</span><span class="p">,</span><span class="s">'Balance'</span><span class="p">,</span><span class="s">'ShotPower'</span><span class="p">,</span><span class="s">'Strength'</span><span class="p">,</span><span class="s">'LongShots'</span><span class="p">,</span><span class="s">'Aggression'</span><span class="p">,</span><span class="s">'Interceptions'</span><span class="p">]</span>
<span class="n">categorical_cols</span> <span class="o">=</span> <span class="p">[</span><span class="s">'Preferred Foot'</span><span class="p">,</span><span class="s">'Position'</span><span class="p">,</span><span class="s">'Body Type'</span><span class="p">,</span><span class="s">'Nationality'</span><span class="p">,</span><span class="s">'Weak Foot'</span><span class="p">]</span>
<span class="n">player_df</span> <span class="o">=</span> <span class="n">player_df</span><span class="p">[</span><span class="n">numerical_cols</span><span class="o">+</span><span class="n">categorical_cols</span><span class="p">]</span>
<span class="n">traindf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">player_df</span><span class="p">[</span><span class="n">numerical_cols</span><span class="p">],</span> <span class="n">pd</span><span class="o">.</span><span class="n">get_dummies</span><span class="p">(</span><span class="n">player_df</span><span class="p">[</span><span class="n">categorical_cols</span><span class="p">])],</span><span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
<span class="n">features</span> <span class="o">=</span> <span class="n">traindf</span><span class="o">.</span><span class="n">columns</span>
<span class="n">traindf</span> <span class="o">=</span> <span class="n">traindf</span><span class="o">.</span><span class="n">dropna</span><span class="p">()</span>
<span class="n">traindf</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">traindf</span><span class="p">,</span><span class="n">columns</span><span class="o">=</span><span class="n">features</span><span class="p">)</span>

<span class="c1"># goals &gt;= 87 =&gt; great player
</span><span class="n">y</span> <span class="o">=</span> <span class="n">traindf</span><span class="p">[</span><span class="s">'Overall'</span><span class="p">]</span><span class="o">&gt;=</span><span class="mi">87</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">traindf</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="k">del</span> <span class="n">X</span><span class="p">[</span><span class="s">'Overall'</span><span class="p">]</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">rename</span><span class="p">(</span><span class="n">columns</span> <span class="o">=</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span><span class="n">re</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">'[^A-Za-z0-9_]+'</span><span class="p">,</span> <span class="s">''</span><span class="p">,</span> <span class="n">x</span><span class="p">))</span> <span class="c1"># lightGBS doesn't handle non ascii characters
</span>
<span class="c1"># set parameters
</span><span class="n">feature_name</span><span class="o">=</span><span class="nb">list</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="p">)</span>
<span class="n">max_feats_num</span> <span class="o">=</span> <span class="mi">30</span>
<span class="n">X_norm</span> <span class="o">=</span> <span class="n">MinMaxScaler</span><span class="p">()</span><span class="o">.</span><span class="n">fit_transform</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="c1"># Transform features by scaling each feature to a given range
</span>
</code></pre></div></div> <h4 id="feature-importance-methods">Feature Importance Methods:</h4> <h2 id="1-pearson-correlation">1. Pearson Correlation</h2> <p><strong>Filter based method.</strong></p> <p>The Pearson correlation coefficient to measure the strength of the relationship between two variables is defined as:</p> <p>$r=\frac{\sum(x_t-\overline{x})(y_t-\overline{y})}{\sqrt{\sum(x_t-\overline{x})^2}\sqrt{\sum(y_t-\overline{y})^2}}$</p> <p>We check the absolute value of the Pearson’s correlation between the target and numerical features in our dataset. We keep the top n features based on this criterion:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">pears_correl_selector</span><span class="p">(</span><span class="n">X</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="n">max_feats_num</span><span class="p">:</span><span class="nb">int</span><span class="p">):</span>
    <span class="s">"""
    Filter based selection.
    Check the absolute value of the Pearson’s correlation between the target and numerical features in our dataset.
    Return the top max_feats_num features.
    
    Returns
    --------
    cor_feature: list
    """</span>
    <span class="n">cor_list</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">feature_name</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="c1"># calculate the correlation with y for each feature
</span>    <span class="k">for</span> <span class="n">feature</span> <span class="ow">in</span> <span class="n">feature_name</span><span class="p">:</span>
        <span class="n">corellation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">corrcoef</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">feature</span><span class="p">],</span> <span class="n">y</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">]</span>
        <span class="n">cor_list</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">corellation</span><span class="p">)</span>
    <span class="c1"># replace NaN with 0
</span>    <span class="n">cor_list</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">cor</span><span class="p">)</span> <span class="k">else</span> <span class="n">cor</span> <span class="k">for</span> <span class="n">cor</span> <span class="ow">in</span> <span class="n">cor_list</span><span class="p">]</span>
    <span class="c1"># feature name
</span>    <span class="n">cor_feature</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">iloc</span><span class="p">[:,</span><span class="n">np</span><span class="o">.</span><span class="n">argsort</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="nb">abs</span><span class="p">(</span><span class="n">cor_list</span><span class="p">))[</span><span class="o">-</span><span class="n">max_feats_num</span><span class="p">:]]</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="c1"># feature selection? 0 for not select, 1 for select
</span>    <span class="n">cor_support</span> <span class="o">=</span> <span class="p">[</span><span class="bp">True</span> <span class="k">if</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">cor_feature</span> <span class="k">else</span> <span class="bp">False</span> <span class="k">for</span> <span class="n">feat</span> <span class="ow">in</span> <span class="n">feature_name</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">cor_feature</span><span class="p">,</span> <span class="n">cor_support</span>
</code></pre></div></div> <h2 id="2-chi-squared-statistical-test">2. Chi-Squared Statistical Test</h2> <p><strong>Filter based method.</strong></p> <p>We calculate the chi-square metric between the target and the numerical variable and only select the variable with the maximum chi-squared values. If a feature is independent to the target it is uniformative for classifying observations.</p> <p>$X^2=\sum_{i=1}^n\frac{(O_i-E_i)^2}{E_i}$</p> <p>$O_i$: the number of observations in a class $i$</p> <p>$E_i$: the number of expected observations in class $i$ if there was no relationship between the feature and target</p> <p>We generally calculate the squares as they willl eventually tell us if the difference between the expected and observation is statistically significant. It also works in a hand-wavy way with non-negative numerical and categorical features.</p> <p>Find the code bellow:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">chi_squared_selector</span><span class="p">(</span><span class="n">X</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">X_norm</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="n">max_feats_num</span><span class="p">:</span><span class="nb">int</span><span class="p">):</span>
    <span class="s">"""
    Calculate the chi-square metric between the target and the numerical variable.
    Return the max_feats_num variables with the maximum chi-squared values
    
    Return
    --------
    chi_feature: list
    """</span>
    <span class="n">chi_selector</span> <span class="o">=</span> <span class="n">SelectKBest</span><span class="p">(</span><span class="n">chi2</span><span class="p">,</span> <span class="n">max_feats_num</span><span class="p">)</span> <span class="c1"># Select features according to the k highest scores.
</span>    <span class="n">chi_selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_norm</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">chi_support</span> <span class="o">=</span> <span class="n">chi_selector</span><span class="o">.</span><span class="n">get_support</span><span class="p">()</span> <span class="c1">#Get a mask, or integer index, of the features selected
</span>    <span class="n">chi_feature</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">chi_support</span><span class="p">]</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">chi_feature</span><span class="p">,</span> <span class="n">chi_support</span>
</code></pre></div></div> <h2 id="3-recursive-feature-elimination">3. Recursive Feature Elimination</h2> <p><strong>Wrapper based method.</strong> Considers the selection of features as a set of features as a search problem.</p> <p>The <code class="language-plaintext highlighter-rouge">sklearn</code> RSE: The goal is to select features by <strong>recursively considering smaller and smaller sets of features</strong>. First, the estimator is trained on the initial set of features and the importance of each feature is obtained either through a <code class="language-plaintext highlighter-rouge">coef_ attribute</code> or through a <code class="language-plaintext highlighter-rouge">feature_importances_ attribute</code>. Then, the least important features are pruned from current set of features. That procedure is recursively repeated on the pruned set <strong>until the desired number of features to select</strong> is eventually reached.</p> <p>We can use any estimator with the method. Here, we use <code class="language-plaintext highlighter-rouge">LogisticRegression</code>, and the RFE observes the <code class="language-plaintext highlighter-rouge">coef_ attribute</code> of the <code class="language-plaintext highlighter-rouge">LogisticRegression</code> object:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">rfe_selector</span><span class="p">(</span><span class="n">X</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">X_norm</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="n">max_feats_num</span><span class="p">:</span><span class="nb">int</span><span class="p">):</span>
    <span class="s">"""
    Wrapper based selection. Recursively consider smaller and smaller sets of features until the max_feats_num number of features is eventually reached.
    Returns a list of the top max_feats_num features.
    
    Return
    --------
    rfe_feature: list
    """</span>
    <span class="n">rfe_selector</span> <span class="o">=</span> <span class="n">RFE</span><span class="p">(</span><span class="n">estimator</span><span class="o">=</span><span class="n">LogisticRegression</span><span class="p">(),</span> <span class="n">n_features_to_select</span><span class="o">=</span><span class="n">max_feats_num</span><span class="p">,</span> <span class="n">step</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="mi">5</span><span class="p">)</span>
    <span class="n">rfe_selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_norm</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">rfe_support</span> <span class="o">=</span> <span class="n">rfe_selector</span><span class="o">.</span><span class="n">get_support</span><span class="p">()</span>
    <span class="n">rfe_feature</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">rfe_support</span><span class="p">]</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">rfe_feature</span><span class="p">,</span> <span class="n">rfe_support</span>
</code></pre></div></div> <h2 id="4-lasso-select-from-model">4. Lasso: Select from Model</h2> <p><strong>Embedded method</strong>. Model with a built-in feature selection method. Definition:</p> <table> <tbody> <tr> <td>$a\sum_{i=1}^n</td> <td>w_i</td> <td>$</td> </tr> </tbody> </table> <p>Notes:</p> <ol> <li> <p>Lasso Regressor uses <strong>L1 Norm</strong> as regulizer: the sum of the magnitudes of the vectors in a space. It is the sum of absolute difference of the components of the vectors. Adds penalty equivalent to absolute value of the magnitude of coefficients (many weights are forced to zero, but the ‘relevant’ variables are allowed to have nonzero weights). This peranly factor determines how many features are retained; using cross-validation to choose the penalty factor helps assure that the model will generalize well to future data samples. The degree of sparsity is controlled by the penality term, and some procedure must be used to select it (cross-validation is a common choice).</p> </li> <li>Unlike <a href="https://www.analyticsvidhya.com/blog/2016/01/ridge-lasso-regression-python-complete-tutorial/">Ridge Regression</a>, lasso’s norm regularizer drives parameters to zero.</li> <li>Higher values of <strong>alpha</strong> means fewer features have non-zero values</li> </ol> <p>The code is the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">lasso_selector</span><span class="p">(</span><span class="n">X</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">X_norm</span><span class="p">:</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="n">max_feats_num</span><span class="p">:</span><span class="nb">int</span><span class="p">):</span>
    <span class="s">"""
    Embeded feature selection. Use **L1 Norm** as regulizer.
    
    Return
    --------
    rfe_feature: list
    """</span>
    <span class="n">lr_selector</span> <span class="o">=</span> <span class="n">SelectFromModel</span><span class="p">(</span><span class="n">LogisticRegression</span><span class="p">(</span><span class="n">penalty</span><span class="o">=</span><span class="s">"l1"</span><span class="p">,</span> <span class="n">solver</span><span class="o">=</span><span class="s">'liblinear'</span><span class="p">),</span> <span class="n">max_features</span><span class="o">=</span><span class="n">max_feats_num</span><span class="p">)</span> <span class="c1"># select lasso by setting penalty=1
</span>    <span class="n">lr_selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_norm</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">lr_support</span> <span class="o">=</span> <span class="n">lr_selector</span><span class="o">.</span><span class="n">get_support</span><span class="p">()</span>
    <span class="n">lr_feature</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">lr_support</span><span class="p">]</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">lr_feature</span><span class="p">,</span> <span class="n">lr_support</span>
</code></pre></div></div> <h2 id="5-random-forest">5. Random Forest</h2> <p><strong>Embeded method.</strong></p> <p>We can make use of the Random Forest classifier from <code class="language-plaintext highlighter-rouge">scikit-learn</code>. In this case, we exploit the <code class="language-plaintext highlighter-rouge">feature_importance_</code> feature of the algorithm. The feature importance is calculating by using node impurities in each decision tree. In Random forest, the final feature importance is the average of all decision tree feature importance. The code is the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">random_forest_selector</span><span class="p">(</span><span class="n">X</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="n">max_feats_num</span><span class="p">:</span><span class="nb">int</span><span class="p">):</span>
    <span class="s">"""
    Embeded feature selection. Calculate feature importance using node impurities in each decision tree.
    
    Return
    --------
    rf_feature_feature: list
    """</span>
    <span class="n">rf_selector</span> <span class="o">=</span> <span class="n">SelectFromModel</span><span class="p">(</span><span class="n">RandomForestClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">100</span><span class="p">),</span> <span class="n">max_features</span><span class="o">=</span><span class="n">max_feats_num</span><span class="p">)</span>
    <span class="n">rf_selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">rf_support</span> <span class="o">=</span> <span class="n">rf_selector</span><span class="o">.</span><span class="n">get_support</span><span class="p">()</span>
    <span class="n">rf_feature</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span> <span class="n">rf_support</span><span class="p">]</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">rf_feature</span><span class="p">,</span> <span class="n">rf_support</span>
</code></pre></div></div> <h2 id="6-light-gbm">6. Light GBM</h2> <p><strong>Embeded method.</strong> LightGBM is Microsoft’s solution to <strong>gradient boosting</strong>. It is designed to be distributed and efficient.</p> <p>The code is the following:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">lightGBM_selector</span><span class="p">(</span><span class="n">X</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span><span class="n">pd</span><span class="o">.</span><span class="n">Series</span><span class="p">,</span> <span class="n">max_feats_num</span><span class="p">:</span><span class="nb">int</span><span class="p">):</span>
    <span class="s">"""
    Embeded feature selection. Used XGBoost as a baseline with improved performance/
    
    Return
    --------
    lgb_feature_feature: list
    """</span>
    
    <span class="n">lgbc</span><span class="o">=</span><span class="n">LGBMClassifier</span><span class="p">(</span><span class="n">n_estimators</span><span class="o">=</span><span class="mi">500</span><span class="p">,</span> <span class="n">learning_rate</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">num_leaves</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">colsample_bytree</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
            <span class="n">reg_alpha</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">reg_lambda</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">min_split_gain</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">40</span><span class="p">)</span>
    <span class="n">lgb_selector</span> <span class="o">=</span> <span class="n">SelectFromModel</span><span class="p">(</span><span class="n">lgbc</span><span class="p">,</span> <span class="n">max_features</span><span class="o">=</span><span class="n">max_feats_num</span><span class="p">)</span>
    <span class="n">lgb_selector</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">lgb_support</span> <span class="o">=</span> <span class="n">lgb_selector</span><span class="o">.</span><span class="n">get_support</span><span class="p">()</span>
    <span class="n">lgb_feature</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">loc</span><span class="p">[:,</span><span class="n">lgb_support</span><span class="p">]</span><span class="o">.</span><span class="n">columns</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">lgb_feature</span><span class="p">,</span> <span class="n">lgb_support</span>
</code></pre></div></div> <h2 id="comparing-important-feature-sets">Comparing Important Feature Sets</h2> <p>We can compare all the methods mentioned above by having all the results gathered in a dataset:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">create_feature_selection_df</span><span class="p">(</span><span class="n">feature_name</span><span class="p">,</span> <span class="n">pearson_cor_support</span><span class="p">,</span> <span class="n">chi_squared_support</span><span class="p">,</span> <span class="n">rfe_support</span><span class="p">,</span> <span class="n">lasso_support</span><span class="p">,</span> <span class="n">rf_support</span><span class="p">,</span> <span class="n">lgb_support</span><span class="p">,</span> <span class="n">max_feats_num</span><span class="p">):</span>
    <span class="n">feature_selection_df</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s">'Feature'</span><span class="p">:</span><span class="n">feature_name</span><span class="p">,</span> <span class="s">'Pearson'</span><span class="p">:</span><span class="n">cor_support</span><span class="p">,</span> <span class="s">'Chi-2'</span><span class="p">:</span><span class="n">chi_squared_support</span><span class="p">,</span> <span class="s">'RFE'</span><span class="p">:</span><span class="n">rfe_support</span><span class="p">,</span> <span class="s">'Logistics'</span><span class="p">:</span> <span class="n">lasso_support</span><span class="p">,</span>
                                    <span class="s">'Random Forest'</span><span class="p">:</span><span class="n">rf_support</span><span class="p">,</span> <span class="s">'LightGBM'</span><span class="p">:</span><span class="n">lgb_support</span><span class="p">})</span>
    <span class="c1"># count the selected times for each feature
</span>    <span class="n">feature_selection_df</span><span class="p">[</span><span class="s">'Total'</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="nb">sum</span><span class="p">(</span><span class="n">feature_selection_df</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="c1"># display the top 100
</span>    <span class="n">feature_selection_df</span> <span class="o">=</span> <span class="n">feature_selection_df</span><span class="o">.</span><span class="n">sort_values</span><span class="p">([</span><span class="s">'Total'</span><span class="p">,</span><span class="s">'Feature'</span><span class="p">]</span> <span class="p">,</span> <span class="n">ascending</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
    <span class="n">feature_selection_df</span><span class="o">.</span><span class="n">index</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">feature_selection_df</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">feature_selection_df</span>
</code></pre></div></div> <p>Now we have all the results in a dataframe and we check if we get a feature based on all the methods. On top of this implementation we could create an evaluation function that finds the best feature selection algorithm for a specific dataset. In the next update of this article I plan to another algorithm that does that.</p> <h2 id="evaluating-the-feature-selection-methods">Evaluating the Feature Selection Methods</h2> <p>Since some prediction models work better with specific feature selection methods we need to carefully select the feature selection method to best fit our model. We also need to measure the efficiency of the feature selection.</p> <p>Approach: For all the methods available, we can calculate the <strong>mean squared log error</strong> of the prediction and create a table of feature selection method against prediction methods errors. We can use the following ways of intepreting the results:</p> <ol> <li>Minimum mean squared log error value</li> <li>Ranking the feature selection method with the highest score in the column</li> <li>Ranking the average of feature selection methods scores.</li> <li>Δ between the mean squared log error and the minimum mean squared log error of the models. Further investigation on this uncovers models with relatively bad results tend to skew the selection process towards the algorithms that performed best for them. To counter this affect we can adjust the weight of the models according to their distance from the absolute minimum:</li> </ol> <p>The distance:</p> <p>$(1-\frac{PM_{min}}{PM})*(\frac{Absolute Min}{FSM_{min}})$</p> <p>$FSM_{min}$: The minimum mean squared log error of the FSM</p> <p>$Absolute Min$: The minimum mean squared log error at the table FSM/PM</p> <p>$PM_{min}$: The minimum mean squared log error of the prediction method</p> <p>$PM$: The minimum mean squared log error</p> <p>The final results might be the same, But FSMs with higher $FSM_{min}$ have less influence (weight) on the average and there for, on the FSM selected. This way we give a chance to all of the FSMs but not in away that can bias the results by large 𝛥 from the Minimum.</p> <p>The algorithm:</p> <ol> <li>Calculate the mean squared log error (msle) for all feature selection methods with all prediction models.</li> <li>For each Prediction model select the FSM/PM with the minimum msle</li> <li>Calculate the distance (adjusting the weight of the PMs according to their distance from the absolute minimum)</li> <li>Calculate the average for each FSM</li> <li>Select the FSM with the minimum average and set the FSM as the selected best FSM</li> </ol> <p>sources:</p> <ol> <li><a href="https://towardsdatascience.com/the-5-feature-selection-algorithms-every-data-scientist-need-to-know-3a6b566efd2">The 5 Feature Selection Algorithms every Data Scientist should know</a></li> <li><a href="https://www.kaggle.com/mlwhiz/feature-selection-using-football-data">Feature Selection Using Football Data</a></li> <li><a href="https://chrisalbon.com/">Notes On Using Data Science &amp; Machine Learning To Fight For Something That Matters </a></li> <li><a href="https://www.youtube.com/watch?v=7_cs1YlZoug">Chi-Square Tests: Crash Course Statistics #29</a></li> <li><a href="https://stats.stackexchange.com/questions/367155/why-lasso-for-feature-selection">Why LASSO for feature selection?</a></li> <li><a href="https://lightgbm.readthedocs.io/en/latest/">Welcome to LightGBM’s documentation!</a></li> <li><a href="https://towardsdatascience.com/feature-selection-evaluation-for-automated-ai-e67f488098d8">Feature selection evaluation for automated AI</a></li> </ol> </div> <div id="disqus_thread"></div> </article> <script> var disqus_config = function () { this.page.url = "/datascience/2020/07/17/feature_selection/"; /* Replace PAGE_URL with your page's canonical URL variable */ this.page.identifier = "/datascience/2020/07/17/feature_selection"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */ }; (function () { /* DON'T EDIT BELOW THIS LINE */ var d = document, s = d.createElement('script'); s.src = 'https://.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a> </noscript></div> </div> <!-- End of row--> <div class="row"> <div class="col-md-4"> <div class="card"> <div class="card-header"> About </div> <div class="card-body"> <!-- Your Bio --> <p class="author_bio"> Hello, My Name is Maria.</p> </div> </div> </div> <div class="col-md-4"> <div class="card"> <div class="card-header">Categories </div> <div class="card-body text-dark"> <div id="#guides"></div> <li class="tag-head"> <a href="/blog/categories/guides">guides</a> </li> <a name="guides"></a> <div id="#dataScience"></div> <li class="tag-head"> <a href="/blog/categories/dataScience">dataScience</a> </li> <a name="dataScience"></a> <div id="#devops"></div> <li class="tag-head"> <a href="/blog/categories/devops">devops</a> </li> <a name="devops"></a> <div id="#resources"></div> <li class="tag-head"> <a href="/blog/categories/resources">resources</a> </li> <a name="resources"></a> <div id="#AI"></div> <li class="tag-head"> <a href="/blog/categories/AI">AI</a> </li> <a name="AI"></a> </div> </div> </div> <div class="col-md-4"> <div class="card"> <div class="card-header">Useful Links </div> <div class="card-body text-dark"> <li > <a href="/about">About Me</a> </li> <li > <a href="/blog">Blog</a> </li> </div> </div> </div> </div> </div> </div> <footer> <p> Powered by<a href="https://github.com/sujaykundu777/devlopr-jekyll"> devlopr jekyll</a>. Hosted at <a href="https://pages.github.com">Github</a>. Subscribe via <a href=" /feed.xml ">RSS</a> </p> </footer> </div> <script> var options = { classname: 'my-class', id: 'my-id' }; var nanobar = new Nanobar( options ); nanobar.go( 30 ); nanobar.go( 76 ); nanobar.go(100); </script> <div hidden id="snipcart" data-api-key="Y2I1NTAyNWYtMTNkMy00ODg0LWE4NDItNTZhYzUxNzJkZTI5NjM3MDI4NTUzNzYyMjQ4NzU0"></div> <script src="https://cdn.snipcart.com/themes/v3.0.0-beta.3/default/snipcart.js" defer></script> <script src="/assets/js/mode-switcher.js"></script> </body> </html>
