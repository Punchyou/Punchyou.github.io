<!DOCTYPE html> <html lang=" en "><head> <meta charset="utf-8"> <title>Maria Pantsiou - Software Developer @ARM</title> <meta http-equip="X-UA-Compatible" content="IE=edge"> <meta name="viewport" content="width=device-width, initial-scale=1"> <meta name="description" content="Machine Learning in Production" /> <meta name="keywords" content="Machine Learning in Production, Maria Pantsiou, AI" /> <link rel="alternate" type="application/rss+xml" title="RSS" href="/feed.xml"> <meta content="" property="fb:app_id"> <meta content="Maria Pantsiou" property="og:site_name"> <meta content="Machine Learning in Production" property="og:title"> <meta content="article" property="og:type"> <meta content="Machine Learning in Production" property="og:description"> <meta content="/ai/2021/02/08/machine_learning_in_production/" property="og:url"> <meta content="2021-02-08T09:05:23+00:00" property="article:published_time"> <meta content="/about/" property="article:author"> <meta content="AI" property="article:section"> <meta name="twitter:card" content="summary"> <meta name="twitter:site" content="@"> <meta name="twitter:creator" content="@"> <meta name="twitter:title" content="Machine Learning in Production"> <meta content="Maria Pantsiou" property="og:site_name"> <meta name="twitter:url" content="/ai/2021/02/08/machine_learning_in_production/"> <meta name="twitter:description" content="devlopr-jekyll is a Jekyll Theme Built For Developers"> <link rel="stylesheet" href="/assets/css/main.css" /> <link rel="stylesheet" href="/assets/css/custom-style.css" /> <link rel="stylesheet" href="/assets/bower_components/lightgallery/dist/css/lightgallery.min.css"/> <link rel="stylesheet" href="https://cdn.snipcart.com/themes/v3.0.0-beta.3/default/snipcart.css" /> <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch.min.css"> <link rel="stylesheet" type="text/css" href="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch-theme-algolia.min.css"> <link rel="stylesheet" href="/assets/bower_components/bootstrap/dist/css/bootstrap.min.css" /> <link rel="stylesheet" href="/assets/bower_components/font-awesome/web-fonts-with-css/css/fontawesome-all.min.css" /> <!-- Fonts--> <link href="https://fonts.googleapis.com/css?family=Quicksand&display=swap" rel="stylesheet"> <!-- Favicon --> <link rel="icon" href="/assets/img/favicon.ico" type="image/gif" sizes="16x16"> <!-- Jquery --> <script src="https://code.jquery.com/jquery-3.4.1.min.js" integrity="sha384-vk5WoKIaW/vJyUAd9n/wmopsmNhiy+L2Z+SBxGYnUkunIxVxAv/UtMOhba/xskxh" crossorigin="anonymous"></script> <!-- <script src="/assets/bower_components/jquery/dist/jquery.min.js"></script> --> <script src="/assets/bower_components/jquery.easing/jquery.easing.min.js"></script> <script src="/assets/bower_components/bootstrap/dist/js/bootstrap.bundle.min.js"></script> <script src="/assets/bower_components/jquery-mousewheel/jquery.mousewheel.min.js"></script> <script src="/assets/bower_components/lightgallery/dist/js/lightgallery-all.min.js"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/picturefill/3.0.2/picturefill.min.js"></script> <script src="/assets/bower_components/imagesloaded/imagesloaded.pkgd.min.js"></script> <script src="/assets/bower_components/nanobar/nanobar.min.js"></script> <script src="/assets/bower_components/typewrite/dist/typewrite.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/instantsearch.js@2.6.0/dist/instantsearch.min.js"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/moment.js/2.20.1/moment.min.js"></script> <!-- Github Button --> <script async defer src="https://buttons.github.io/buttons.js"></script> <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.14.3/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script> <script> (function (i, s, o, g, r, a, m) { i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () { (i[r].q = i[r].q || []).push(arguments) }, i[r].l = 1 * new Date(); a = s.createElement(o), m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m) })(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga'); ga('create', '', 'auto'); ga('send', 'pageview'); </script> </head> <body> <div class="container-fluid"><header> <div class="col-lg-12"> <div class="row"> <nav class="navbar navbar-expand-lg fixed-top navbar-dark " id="topNav"> <!-- <a class="navbar-brand" href="#">Maria Pantsiou</a> --> <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="navbar-toggler-icon"></span> </button> <a class="navbar-brand" href="/">Maria Pantsiou</a> <div class="collapse navbar-collapse" id="navbarNav"> <ul class="navbar-nav"> <li class="nav-item"> <a class="nav-link" href="/about">About Me</a> </li> <li class="nav-item"> <a class="nav-link" href="/blog">Blog</a> </li> </ul> </div> <ul class="nav justify-content-end"> <li class="nav-item"> <a class="nav-link" id="search-icon" href="/search/"><i class="fa fa-search" aria-hidden="true"></i></a> </li> <li class="nav-item"> <input class="nav-link switch" id="theme-toggle" onclick="modeSwitcher() "type="checkbox" name="checkbox" > </li> </ul> </nav> </div> </div> </header><div class="col-lg-12"> <!-- Blog Post Breadcrumbs --><div class="col-lg-12"> <nav aria-label="breadcrumb" role="navigation"> <ol class="breadcrumb"> <li class="breadcrumb-item"> <a href="/blog"><i class="fa fa-home" aria-hidden="true"></i></a> </li> <li class="breadcrumb-item active" aria-current="page"><a href="/ai/2021/02/08/machine_learning_in_production/">Machine Learning in Production</a></li> </ol> </nav> </div> <div class="row" id="blog-post-container"> <div class="col-lg-8 offset-md-2"><article class="card" itemscope itemtype="http://schema.org/BlogPosting"> <div class="card-header"> <!-- <h1 class="post-title" itemprop="name headline">Machine Learning in Production</h1> --> <h4 class="post-meta">Machine Learning in Production</h4> <p class="post-summary">Posted by : <img src="/assets/img/avatar.png" class="author-profile-img"> <span itemprop="author" itemscope itemtype="http://schema.org/Person"> <span itemprop="name">Maria Pantsiou</span> </span> at <time datetime="2021-02-08 09:05:23 +0000" itemprop="datePublished">Feb 8, 2021</time> </p> <span class="disqus-comment-count" data-disqus-identifier="/ai/2021/02/08/machine_learning_in_production/"></span> <div class="post-categories"> Category : <a href="/blog/categories/AI">AI</a> </div> </div> <div class="card-body" itemprop="articleBody"> <img class="card-img-top" src="/assets/img/posts/light_circles2.png" alt=""> <br/> <br/> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], displayMath: [['$$','$$']], inlineMath: [['$','$']], }, }); </script> <h1 id="machine-learning-in-production">Machine Learning in Production</h1> <h2 id="endpoints-and-rest-apis">Endpoints and REST APIs</h2> <p>The interface (endpoint) facilitates an ease of communication between the model and the applicationion</p> <p>One way to think of the endpoint that acts as this interface, is to think of a Python program where:</p> <ul> <li>The endpoint itself is like a function call</li> <li>The function itself would be the model and</li> <li>The Python program is the application.</li> </ul> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="s">""" The whole script is the application"""</span>
<span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
	<span class="n">input_user_data</span> <span class="o">=</span> <span class="n">get_user_data</span><span class="p">()</span>

	<span class="c1"># this is the interface
</span>	<span class="n">predictions</span> <span class="o">=</span> <span class="n">ml_model</span><span class="p">(</span><span class="n">input_user_data</span><span class="p">)</span>


<span class="k">def</span> <span class="nf">ml_model</span><span class="p">(</span><span class="n">data</span><span class="p">):</span>
	<span class="s">""" This is the model"""</span>
	<span class="n">loaded_data</span> <span class="o">=</span> <span class="n">load_user_data</span><span class="p">(</span><span class="n">data</span><span class="p">)</span>
</code></pre></div></div> <h3 id="rest-api">REST API</h3> <p>Communication between the application and the model is done through the endpoint (interface), where the endpoint is an Application Programming Interface (API). The <strong>REST API</strong> is one that uses HTTP requests and responses to enable communication between the application and the model through the endpoint (interface).</p> <ul> <li>Endpoint</li> </ul> <p>This endpoint will be in the form of a URL, Uniform Resource Locator, which is commonly known as a web address.</p> <h3 id="http-methods">HTTP Methods:</h3> <ul> <li>GET: READ request action (retrieve information - of found is returned)</li> <li>POST: CREATE request action (create new info - once is created it’s returned as a reposnse)</li> <li>PUT: UPDATE request action (there is also PATCH for partial update)</li> <li>DELETE: DELETE request action</li> </ul> <p>The HTTP response sent from your model to your application is composed of three parts:</p> <ul> <li>HTTP Status Code</li> </ul> <p>If the model successfully received and processed the user’s data that was sent in the message, the status code should start with a 2, like 200.</p> <ul> <li>HTTP Headers</li> </ul> <p>The headers will contain additional information, like the format of the data within the message, that’s passed to the receiving program.</p> <ul> <li>Message (Data or Body) What’s returned as the data within the message is the prediction that’s provided by the model. The user Data might need to be formatted (csv or json). The HTTP response message might need translations to be readable for the application’s user (like csv or json).</li> </ul> <p>The HTTP request that’s sent from your application to your model uses a POST HTTP Method.</p> <h2 id="containers">Containers</h2> <p>The model and the application can each be run in a container computing environment. The containers are created using a script that contains instructions on which software packages, libraries, and other computing attributes are needed in order to run a software application, in our case either the model or the application.</p> <p>A container can be thought of as a standardized collection/bundle of software that is to be used for the specific purpose of running an application.</p> <p>Three continer running three different applications: <img src="../assets/img/container.png" alt="container" /></p> <p>Example of a <a href="https://github.com/pytorch/pytorch/blob/master/docker/pytorch/Dockerfile">dockerfile</a>, a file of instructions about how a container ccan be created.</p> <p>Notes on containers:</p> <ul> <li>Containers are sharing the kernel with underlying OS, like VMs. They are though lighter.</li> <li>In order to install new software, we can update the build script for the container and run them again.</li> <li>With container we use microservices and we brek down things in smaller and managable units.</li> </ul> <h2 id="production-environment">Production environment</h2> <p>Deployment to production can simply be thought of as a method that integrates a machine learning model into an existing production environment so that the model can be used to make decisions or predictions based upon data input into this model. A production evironment can be a mobile or web application among others.</p> <p>In machine learning, a <strong>hyperparameter</strong> is a parameter whose value cannot be estimated from the data. It is not directly learned through the estimators; therefore, their value must be set by the model developer. Often cloud platform machine learning services provide methods that allow for automatic hyperparameter tuning for use with model training.</p> <h3 id="model-deployment-characteristics">Model Deployment Characteristics</h3> <ul> <li>Model Versioning</li> <li>Model Monitoring</li> <li>Model Updating (from change in the data) and Routing (the deployment platform should support routing differing proportions of user requests to the deployed models; to allow comparison of performance between the deployed model variants.</li> </ul> <p>Routing in this way allows for a test of a model performance as compared to other model variants.)</p> <ul> <li>Model predictions: On-demand (real-time, as responses from a request, needs to have low latency) or Batch prediction (asynchronous, like high volume of requests with periodic submitions)</li> </ul> <h3 id="sagemaker">SageMaker</h3> <ul> <li>There are at least fifteen built-in algorithms that are easily used within SageMaker. Contains algorithms like linear learner or XGBoost, item recommendations using factorization machine, grouping based upon attributes using K-Means and more. You can also create custome algorithms, using popular ML algorithms.</li> <li>Make use of jupyter notebooks</li> <li>Has tuning tools and monitoring</li> <li>You must leave it running to provide predictions, so it’s faster.</li> </ul> <p>Other Notes</p> <ul> <li>TensorFlow can be used for creating, training, and deploying machine learning and deep learning models. Keras is a higher level API written in Python that runs on top of TensorFlow, that’s easier to use and allows for faster development.</li> <li>Scikit-learn and an XGBoost Python package can be used together for creating, training, and deploying machine learning models.</li> </ul> <h2 id="aws-quotas-and-commands">AWS Quotas and commands</h2> <h3 id="list-your-quotas-type-pn-awl-cli">List your quotas, type pn AWL CLI:</h3> <p><code class="language-plaintext highlighter-rouge">list-service-quotas</code> and <code class="language-plaintext highlighter-rouge">list-aws-default-service-quotas</code></p> <h3 id="increase-your-quotas">Increase your quotas</h3> <ul> <li>Use amazon Service Quotas service. This service consolidates your account-specific values for quotas across all AWS services for improved manageability. Service Quotas is available at no additional charge. You can directly try logging into Service Quotas console here.</li> <li>Using AWS Support Center - You can create a case for support from AWS. command: <code class="language-plaintext highlighter-rouge">request-service-quota-increase</code></li> <li><strong>Amazon SageMaker ML Instance Types</strong>:characterized by a combination of CPU, memory, GPU, GPU memory, and networking capacity.</li> <li><strong>Shut Down SageMaker Instances, if not in use</strong>. You can re-install it later.</li> <li>Read about the limits <a href="https://docs.aws.amazon.com/general/latest/gr/sagemaker.html">here</a>.</li> </ul> <h2 id="set-up-instances">Set up Instances</h2> <h3 id="notebooks">Notebooks</h3> <ul> <li>Amazon Saze Maker → Notebook Instances → Creat Notebook Instance</li> <li>Give the notebook a name, and a role (like a security acess level) under <strong>Permissions and Encryption</strong> → Create a new role → S3 buckets you specify: <em>None</em> → Create Notebook Instance</li> <li>Once the notebook instance is ready, amazon will automatically start it → <strong>Stop it if you won’t use it right away</strong> - costs increases by the time</li> </ul> <h4 id="notebooks-with-sagemaker">Notebooks with SageMaker</h4> <ul> <li>When training in notebooks, sagemaker will create VM with the characteristics we chose, then the VM will load an image in the form of a docker container, which contined the code to use XGBoost.</li> <li>The VM needs access to the data, which should be available in S3 (amazon data storage facilities), so need to upload our datasets there (by saving the dataset from the script). Then we upload by using sagemaker package:</li> <li>Sagemaker needs to be installed (can be done within a notebook with <code class="language-plaintext highlighter-rouge">!pip</code>)</li> </ul> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">sagemaker</span>
<span class="n">session</span> <span class="o">=</span> <span class="n">sagemaker</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span>
<span class="n">session</span><span class="o">.</span><span class="n">upload_data</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s">'test.csv'</span><span class="p">),</span> <span class="n">key_prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">)</span>
</code></pre></div></div> <p>This uploads the file to the S3 bucket associated with this session.</p> <p>You can also print your role (defines how data that your notebook uses/creates will be stotred) <code class="language-plaintext highlighter-rouge">print(role)</code></p> <ul> <li>Different containers are created for different regions and models we choose to use. Amazon provides us with a function that can print the container uri, if we pass the region and the model as arguments:</li> </ul> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sagemaker.amazon.amazon_estimator</span> <span class="kn">import</span> <span class="n">get_image_uri</span>
<span class="n">container</span> <span class="o">=</span> <span class="n">get_image_uri</span><span class="p">(</span><span class="n">session</span><span class="o">.</span><span class="n">boto_region_name</span><span class="p">,</span> <span class="s">'xgboost'</span><span class="p">)</span>
</code></pre></div></div> <p>This will return something like:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>'811284229777.dkr.ecr.us-east-1.amazonaws.com/xgboost:1'
</code></pre></div></div> <p>Then we can train our model:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># we create an estimator
# we want the estimator to have the appropriate role to access the data
</span><span class="n">xgb</span> <span class="o">=</span> <span class="n">sagemaker</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">Estimator</span><span class="p">(</span><span class="n">container</span><span class="p">,</span> <span class="c1"># The image name of the training container
</span>                                    <span class="n">role</span><span class="p">,</span>      <span class="c1"># The IAM role to use (our current role in this case)
</span>                                    <span class="n">train_instance_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="c1"># The number of instances to use for training
</span>                                    <span class="n">train_instance_type</span><span class="o">=</span><span class="s">'ml.m4.xlarge'</span><span class="p">,</span> <span class="c1"># The type of instance to use for training
</span>                                    <span class="n">output_path</span><span class="o">=</span><span class="s">'s3://{}/{}/output'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">session</span><span class="o">.</span><span class="n">default_bucket</span><span class="p">(),</span> <span class="n">prefix</span><span class="p">),</span>
                                                                        <span class="c1"># Where to save the output (the model artifacts)
</span>                                    <span class="n">sagemaker_session</span><span class="o">=</span><span class="n">session</span><span class="p">)</span> <span class="c1"># The current SageMaker session
</span>
</code></pre></div></div> <p>Then we can set the hyper parameters by:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xgb</span><span class="o">.</span><span class="n">set_hyperparameters</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                        <span class="n">eta</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
                        <span class="n">gamma</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                        <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
                        <span class="n">subsample</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                        <span class="n">objective</span><span class="o">=</span><span class="s">'reg:linear'</span><span class="p">,</span>
                        <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="c1"># in case the model is overfitting and starts to make it worse in the validation set - we should sytop the model then
</span>                        <span class="n">num_round</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
</code></pre></div></div> <p>Learn more about estimators <a href="https://sagemaker.readthedocs.io/en/latest/estimators.html">here</a>. Then we can fit the model, by providing the location of the training/validation datasets:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">s3_input_train</span> <span class="o">=</span> <span class="n">sagemaker</span><span class="o">.</span><span class="n">s3_input</span><span class="p">(</span><span class="n">s3_data</span><span class="o">=</span><span class="n">train_location</span><span class="p">,</span> <span class="n">content_type</span><span class="o">=</span><span class="s">'csv'</span><span class="p">)</span>
<span class="n">s3_input_validation</span> <span class="o">=</span> <span class="n">sagemaker</span><span class="o">.</span><span class="n">s3_input</span><span class="p">(</span><span class="n">s3_data</span><span class="o">=</span><span class="n">val_location</span><span class="p">,</span> <span class="n">content_type</span><span class="o">=</span><span class="s">'csv'</span><span class="p">)</span>

<span class="n">xgb</span><span class="o">.</span><span class="n">fit</span><span class="p">({</span><span class="s">'train'</span><span class="p">:</span> <span class="n">s3_input_train</span><span class="p">,</span> <span class="s">'validation'</span><span class="p">:</span> <span class="n">s3_input_validation</span><span class="p">})</span>
</code></pre></div></div> <p>Lastly we fit the model. We send all the data to sagemaker, and it will decide how to split the data into train/val/test sets.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># create the transformer
</span><span class="n">xgb_transformer</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span><span class="n">instance_count</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">instance_type</span> <span class="o">=</span> <span class="s">'ml.m4.xlarge'</span><span class="p">)</span>
<span class="c1"># start the transform job
</span><span class="n">xgb_transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_location</span><span class="p">,</span> <span class="n">content_type</span><span class="o">=</span><span class="s">'text/csv'</span><span class="p">,</span> <span class="n">split_type</span><span class="o">=</span><span class="s">'Line'</span><span class="p">)</span>
<span class="c1"># check how the transformation jos is progressing
</span><span class="n">xgb_transformer</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
</code></pre></div></div> <p>After the training is complete, we need to bring the output to our notebook. We’ll use AWS’s functionality to do that (still inside the notebook):</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!aws s3 cp --recursive $xgb_transformer.output_path $data_dir
</code></pre></div></div> <p>At the end we should remove the directory with the data (saved in <code class="language-plaintext highlighter-rouge">data_dir</code>), to free up space for the rest of our prpojects:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code># First we will remove all of the files contained in the data_dir directory
!rm $data_dir/*

# And then we delete the directory itself
!rmdir $data_dir
</code></pre></div></div> <h2 id="launch-a-project">Launch a project</h2> <p>An example is <a href="https://github.com/udacity/sagemaker-deployment">here</a>.</p> <h3 id="modeling-on-sagemaker">Modeling on SageMaker</h3> <p>Notes:</p> <ul> <li>XGBoost is a tree based method, so prone to overfitting. Having a validation set for those kind of models can improve results.</li> <li>After uploading data in S3, the files can be found here: AWS → S3 → Choose bucket. There should be the folder with the datasets uploaded.uploaded</li> <li>A sagemaker model is a collection of information that includes both a link to the model artifacts (saved files created by the training job, if we were fitting a linear model then the coefficients that were fit would be saved as model artifacts) and some information in how those srtifacts should be used. We can also add all the nessesary params in a dictionary and unpack them in the job func:</li> <li>We can create training jobs. After a trining job is completed, we can build a sagemaker model.</li> <li>After the model is complete, we can test it, by using transform (like before).</li> </ul> <p>Training job:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">training_job</span> <span class="o">=</span> <span class="n">sessions</span><span class="o">.</span><span class="n">sagemaker_client</span><span class="o">.</span><span class="n">create_training_job</span><span class="p">(</span><span class="o">**</span><span class="n">training_params</span><span class="p">)</span>
<span class="c1"># to check the logs of the session
</span><span class="n">session</span><span class="o">.</span><span class="n">logs_for_job</span><span class="p">(</span><span class="n">training_job_name</span><span class="p">)</span>
</code></pre></div></div> <p>Testing job:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># use the transform_request dict with all the informtion about the continer, VM and model
</span><span class="n">transform_response</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">agemaker_client</span><span class="o">.</span><span class="n">create_transform_job</span><span class="p">(</span><span class="o">**</span><span class="n">transform_request</span><span class="p">)</span>
<span class="c1"># check the logs - transform name to be unique, str
</span><span class="n">transform_desk</span> <span class="o">=</span> <span class="n">ession</span><span class="o">.</span><span class="n">wait_for_transform_job</span><span class="p">(</span><span class="n">transform_job_name</span><span class="p">)</span>
</code></pre></div></div> <h4 id="what-happend-when-a-model-is-fit-using-sagemaker">What happend when a model is fit using SageMaker?</h4> <p>When a model is fit using SageMaker, the process is as follows.</p> <p>First, a compute instance (basically a server somewhere) is started up with the properties that we specified.</p> <p>Next, when the compute instance is ready, the code, in the form of a container, that is used to fit the model is loaded and executed. When this code is executed, it is provided access to the training (and possibly validation) data stored on S3.</p> <p>Once the compute instance has finished fitting the model, the resulting model artifacts are stored on S3 and the compute instance is shut down.</p> <p>You can check the jobs in the AWS console: Underneath the Notebook Instances in AWS Sagemaker, click om Training Jobs. Click on a training job, will reaveal information about the job.There, theres is also a <code class="language-plaintext highlighter-rouge">View logs</code> option.</p> <h2 id="deploy-a-model-with-sagemaker">Deploy a model with SageMaker</h2> <p>Instead of testing the model with the transform method, we will deploy our model and then send the test data to the endpoint. Deployment means creating an endpoint, which is just a URL that we can send data to. The only thing we need to do to deploy it in an application, is to just call <code class="language-plaintext highlighter-rouge">deploy()</code> in our existing model (after training). This will start the endpoint and the <strong>charging</strong> will start:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># we need to pass the number of VMs we want to use, and the type of the VM
</span><span class="n">xgb_preedictor</span> <span class="o">=</span> <span class="n">xgb</span><span class="o">.</span><span class="n">deploy</span><span class="p">(</span><span class="n">initial_instance_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">instance_type</span><span class="o">=</span><span class="s">'ml.m4.xlarge'</span><span class="p">)</span>
</code></pre></div></div> <p>SageMaker has now created a VM that has ran the trained model, that it now accessed by an endpoint (URL).</p> <p>In order to send data to the model’s endpoint, we can use the <code class="language-plaintext highlighter-rouge">predict</code> method of the xgd_predictor object that returned above. The data the we provide to the predictor object need to serialized first. We can do that with the sagemaker’s <code class="language-plaintext highlighter-rouge">csv_serializer</code>. the returned results is also serialized (a string), so we’ll need to convert it back to numpy.array:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># We need to tell the endpoint what format the data we are sending is in
</span><span class="n">xgb_predictor</span><span class="o">.</span><span class="n">content_type</span> <span class="o">=</span> <span class="s">'text/csv'</span>
<span class="n">xgb_predictor</span><span class="o">.</span><span class="n">serializer</span> <span class="o">=</span> <span class="n">csv_serializer</span>

<span class="n">Y_pred</span> <span class="o">=</span> <span class="n">xgb_predictor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="o">.</span><span class="n">values</span><span class="p">)</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>
<span class="c1"># predictions is currently a comma delimited string and so we would like to break it up
# as a numpy array.
</span><span class="n">Y_pred</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">fromstring</span><span class="p">(</span><span class="n">Y_pred</span><span class="p">,</span> <span class="n">sep</span><span class="o">=</span><span class="s">','</span><span class="p">)</span>
</code></pre></div></div> <p>Lastly <strong>we need to shut down the deployed model</strong>, as it will run, waiting for data to be send to it:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xgb_predictor</span><span class="o">.</span><span class="n">delete_endpoint</span><span class="p">()</span>
</code></pre></div></div> <p>To do the above in mode detail (configure the endpoint), go to the low-level deployment tutorial of the course. Endpoit configuration objects are also available in <strong>SageMaker menu</strong> → <strong>Endpoint configurations</strong>. After its creation, we can use the same configuration as many times as we want.</p> <p>Once the endpoint is created, we can see it Under <strong>SageMaker menu</strong> → <strong>Endpoints</strong>, where we can see the URL, or delete our endpoint. Then we can test the model (send the data to endpoint) Notes</p> <ul> <li>SageMaker gets the data to the endpoint, and if we want it to, split the data amongst different models.</li> <li>In case you need to aplit the data before feeding them into the model, use : ```py <h1 id="we-split-the-data-into-chunks-and-send-each-chunk-seperately-accumulating-the-results">We split the data into chunks and send each chunk seperately, accumulating the results.</h1> </li> </ul> <p>def predict(data, rows=512): split_array = np.array_split(data, int(data.shape[0] / float(rows) + 1)) predictions = ‘’ for array in split_array: predictions = ‘,’.join([predictions, xgb_predictor.predict(array).decode(‘utf-8’)])</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>return np.fromstring(predictions[1:], sep=',') ```
</code></pre></div></div> <h1 id="hyperparameters-tuning">Hyperparameters Tuning</h1> <ul> <li>SageMaker can run a bunch of models and choose the best one. We have to define how many models to runand the best model method (like rmse).</li> <li>To check the output of the model, check <strong>Cloud Watch</strong>. (logs I mentioned earlier)</li> </ul> <p>Wee need a model as we have done so far, and we set the hyper parameters like before. This will be our basline model:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># set a baseline object
</span><span class="n">xgb</span><span class="o">.</span><span class="n">set_hyper_parameters</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                        <span class="n">eta</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
                        <span class="n">gamma</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                        <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
                        <span class="n">subsample</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                        <span class="n">objective</span><span class="o">=</span><span class="s">'reg:linear'</span><span class="p">,</span>
                        <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
			<span class="n">num_round</span><span class="o">=</span><span class="mi">200</span><span class="p">)</span>
</code></pre></div></div> <p>Then we create another object that will tune the parameters above and we fit the tuner:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xgb_hyperparameter_tuner</span> <span class="o">=</span> <span class="n">HyperparameterTuner</span><span class="p">(</span><span class="n">estimator</span> <span class="o">=</span> <span class="n">xgb</span><span class="p">,</span> <span class="c1"># The estimator object to use as the basis for the training jobs.
</span>                                               <span class="n">objective_metric_name</span> <span class="o">=</span> <span class="s">'validation:rmse'</span><span class="p">,</span> <span class="c1"># The metric used to compare trained models.
</span>                                               <span class="n">objective_type</span> <span class="o">=</span> <span class="s">'Minimize'</span><span class="p">,</span> <span class="c1"># Whether we wish to minimize or maximize the metric.
</span>                                               <span class="n">max_jobs</span> <span class="o">=</span> <span class="mi">20</span><span class="p">,</span> <span class="c1"># The total number of models to train
</span>                                               <span class="n">max_parallel_jobs</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="c1"># The number of models to train in parallel
</span>                                               <span class="n">hyperparameter_ranges</span> <span class="o">=</span> <span class="p">{</span>
                                                    <span class="s">'max_depth'</span><span class="p">:</span> <span class="n">IntegerParameter</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">12</span><span class="p">),</span>
                                                    <span class="s">'eta'</span>      <span class="p">:</span> <span class="n">ContinuousParameter</span><span class="p">(</span><span class="mf">0.05</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">),</span>
                                                    <span class="s">'min_child_weight'</span><span class="p">:</span> <span class="n">IntegerParameter</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">8</span><span class="p">),</span>
                                                    <span class="s">'subsample'</span><span class="p">:</span> <span class="n">ContinuousParameter</span><span class="p">(</span><span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.9</span><span class="p">),</span>
                                                    <span class="s">'gamma'</span><span class="p">:</span> <span class="n">ContinuousParameter</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">),</span>
                                               <span class="p">})</span>
<span class="c1"># load data tp s3
</span><span class="n">s3_input_train</span> <span class="o">=</span> <span class="n">sagemaker</span><span class="o">.</span><span class="n">s3_input</span><span class="p">(</span><span class="n">s3_data</span><span class="o">=</span><span class="n">train_location</span><span class="p">,</span> <span class="n">content_type</span><span class="o">=</span><span class="s">'csv'</span><span class="p">)</span>
<span class="n">s3_input_validation</span> <span class="o">=</span> <span class="n">sagemaker</span><span class="o">.</span><span class="n">s3_input</span><span class="p">(</span><span class="n">s3_data</span><span class="o">=</span><span class="n">val_location</span><span class="p">,</span> <span class="n">content_type</span><span class="o">=</span><span class="s">'csv'</span><span class="p">)</span>
<span class="c1"># fit the tuner
</span><span class="n">xgb_hyperparameter_tuner</span><span class="o">.</span><span class="n">fit</span><span class="p">({</span><span class="s">'train'</span><span class="p">:</span> <span class="n">s3_input_train</span><span class="p">,</span> <span class="s">'validation'</span><span class="p">:</span> <span class="n">s3_input_validation</span><span class="p">})</span>
</code></pre></div></div> <p>To check the nest model:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xgb_hyperparameter_tuner</span><span class="o">.</span><span class="n">best_training_job</span><span class="p">()</span>
</code></pre></div></div> <p>At the moment we have trained the tuner, but not the initial estimator. In order to do that, we can attach the tuner to the estimator. We can choose the best training model:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xgb_attached</span> <span class="o">=</span> <span class="n">sagemaker</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">Estimator</span><span class="o">.</span><span class="n">attach</span><span class="p">(</span><span class="n">xgb_hyperparameter_tuner</span><span class="o">.</span><span class="n">best_training_job</span><span class="p">())</span>
</code></pre></div></div> <p>Then we can test our best model.</p> <p>If we want to use the API, we can do it by defining more details option for the container and the model, the only difference will be that instead of setting up the parameters of the baseline mode, we will only refer to the ones that will remain static, and later one we can define the ranges for the parameters that will vary.</p> <h2 id="deploy-a-sentiment-analysis-model">Deploy a Sentiment Analysis Model</h2> <h3 id="bag-of-words">Bag of words</h3> <p>In the project, we’ll use the bag-of-words approach to determinte if a movie feedback is positive or negative.</p> <ul> <li>Turn each document (or a sentense) into a vector of numbers: <img src="../assets/img/bag_of_words.png" alt="bag of words vector" /> We can compare two documents based on how many words they have in common. The mathematical repressentation is:</li> </ul> <script type="math/tex; mode=display">a\cdotb = \Sigma(a_0 b_0i + a_1 b_1 + ... + a_n b_n) = 1</script> <p>The greater the product, the mode similar the documents. The issue is that different pairs my end up having the same product. In that case, we the <em>cosine similarity</em>:</p> <script type="math/tex; mode=display">cos(θ) = \frac{a\cdot b}{|a|\cdot|b|}</script> <h2 id="web-app-for-the-model-to-be-deployed">Web app for the model to be deployed</h2> <p>Issues you need to overcome:</p> <ul> <li>The endpoind takes encoded data as input, but the user input on the web app is a string.</li> <li>Security - endpoints provided by sagemaker give access only to users authenticated with AWS credentials.</li> </ul> <p>To overcoms those issues si an overkill for a simple application, so we’ll create a new endpoints ourselves.</p> <p>Structure of the web app: <img src="../assets/img/web_app_structure.png" alt="web app" /></p> <h2 id="set-up-the-app">Set up the App</h2> <p>Instead of creating a server, we can tell AWS to only run a function and get charged only for when the fucntion runs. After creating an endpoint method (IMDB Sentiment Analysis - XGBoost - WebApp tutorial notebook), we invoke the endpoint, then serialize and send the data to it. First, we prepare the data:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">re</span>

<span class="n">test_review</span> <span class="o">=</span> <span class="s">"Nothing but a disgusting materialistic pageant of glistening abed remote control greed zombies</span><span class="err">

</span><span class="s">REPLACE_NO_SPACE = re.compile("</span><span class="p">(</span>\<span class="o">.</span><span class="p">)</span><span class="o">|</span><span class="p">(</span>\<span class="p">;)</span><span class="o">|</span><span class="p">(</span>\<span class="p">:)</span><span class="o">|</span><span class="p">(</span>\<span class="err">!</span><span class="p">)</span><span class="o">|</span><span class="p">(</span>\<span class="s">')|(</span><span class="err">\</span><span class="s">?)|(</span><span class="err">\</span><span class="s">,)|(</span><span class="se">\"</span><span class="s">)|(</span><span class="err">\</span><span class="s">()|(</span><span class="err">\</span><span class="s">))|(</span><span class="err">\</span><span class="s">[)|(</span><span class="err">\</span><span class="s">])")</span><span class="err">
</span><span class="s">REPLACE_WITH_SPACE = re.compile("(&lt;br</span><span class="err">\</span><span class="s">s*/&gt;&lt;br</span><span class="err">\</span><span class="s">s*/&gt;)|(</span><span class="err">\</span><span class="s">-)|(</span><span class="err">\</span><span class="s">/)")</span><span class="err">

</span><span class="s">def review_to_words(review):</span><span class="err">
</span><span class="s">    words = REPLACE_NO_SPACE.sub("", review.lower())</span><span class="err">
</span><span class="s">    words = REPLACE_WITH_SPACE.sub(" ", words)</span><span class="err">
</span><span class="s">    return words</span><span class="err">

</span><span class="s">test_words = review_to_words(test_review)</span><span class="err">

</span><span class="s"># create a bag of words embedding of the test_words</span><span class="err">
</span><span class="s"># words is the string cleaned from html symbols</span><span class="err">
</span><span class="s"># vocabulary is the most fewquently appearing words in a document</span><span class="err">
</span><span class="s">def bow_encoding(words, vocabulary):</span><span class="err">
</span><span class="s">    bow = [0] * len(vocabulary) # Start by setting the count for each word in the vocabulary to zero.</span><span class="err">
</span><span class="s">    for word in words.split():  # For each word in the string</span><span class="err">
</span><span class="s">        if word in vocabulary:  # If the word is one that occurs in the vocabulary, increase its count.</span><span class="err">
</span><span class="s">            bow[vocabulary[word]] += 1</span><span class="err">
</span><span class="s">    return bow</span><span class="err">
</span><span class="s">test_bow = bow_encoding(test_words, vocabulary)</span><span class="err">
</span></code></pre></div></div> <p>To invoke the endpoint, we need to have th data converted to csv format, as this is what that function expects:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">body</span> <span class="o">=</span> <span class="s">','</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">test_bow</span><span class="p">])</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span>
<span class="n">response</span> <span class="o">=</span> <span class="n">runtime</span><span class="o">.</span><span class="n">invoke_endpoint</span><span class="p">(</span><span class="n">EndpointName</span> <span class="o">=</span> <span class="n">xgb_predictor</span><span class="o">.</span><span class="n">endpoint</span><span class="p">,</span> <span class="c1"># The name of the endpoint we created
</span>                                       <span class="n">ContentType</span> <span class="o">=</span> <span class="s">'text/csv'</span><span class="p">,</span>                     <span class="c1"># The data format that is expected
</span>                                       <span class="n">Body</span> <span class="o">=</span> <span class="n">body</span><span class="p">)</span>
</code></pre></div></div> <p>we are most interested in is ‘Body’ object:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">response</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s">'Body'</span><span class="p">]</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>
</code></pre></div></div> <p>Make sure you <strong>shut down the endpoint</strong>:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xgb_predictor</span><span class="o">.</span><span class="n">delete_endpoint</span><span class="p">()</span>
</code></pre></div></div> <h3 id="set-up-the-lamba-fuctnion-a-function-as-a-service">Set up the Lamba fuctnion: a function as a service</h3> <p>We need to give the lamba function permissio to use a SageMaker endpoint.</p> <ul> <li>First we’ll set up a new role for Lamba func:</li> </ul> <p>Amazon Console → Secutiry, Identity and Compiance → IAM → Roles → Create Role → Lamba → Next → Check AmazonSageMalerFullAccess → Next → Create a LambaSageMakerRole.</p> <ul> <li>Then we’ll create a function: Amazon Conseol → Compute → Lamba → Create a function → Author from scratch → Name: sentiment_lamba_function → Runtime: Python 3.6 → Role: Choose an existing role, LambaSageMaKerRole → Create</li> </ul> <p>Scroll down to see the funtion (emty for now). We can copy and paste the function from the notebook:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># We need to use the low-level library to interact with SageMaker since the SageMaker API
# is not available natively through Lambda.
</span><span class="kn">import</span> <span class="nn">boto3</span>

<span class="c1"># And we need the regular expression library to do some of the data processing
</span><span class="kn">import</span> <span class="nn">re</span>

<span class="n">REPLACE_NO_SPACE</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="s">"(</span><span class="err">\</span><span class="s">.)|(</span><span class="err">\</span><span class="s">;)|(</span><span class="err">\</span><span class="s">:)|(</span><span class="err">\</span><span class="s">!)|(</span><span class="se">\'</span><span class="s">)|(</span><span class="err">\</span><span class="s">?)|(</span><span class="err">\</span><span class="s">,)|(</span><span class="se">\"</span><span class="s">)|(</span><span class="err">\</span><span class="s">()|(</span><span class="err">\</span><span class="s">))|(</span><span class="err">\</span><span class="s">[)|(</span><span class="err">\</span><span class="s">])"</span><span class="p">)</span>
<span class="n">REPLACE_WITH_SPACE</span> <span class="o">=</span> <span class="n">re</span><span class="o">.</span><span class="nb">compile</span><span class="p">(</span><span class="s">"(&lt;br</span><span class="err">\</span><span class="s">s*/&gt;&lt;br</span><span class="err">\</span><span class="s">s*/&gt;)|(</span><span class="err">\</span><span class="s">-)|(</span><span class="err">\</span><span class="s">/)"</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">review_to_words</span><span class="p">(</span><span class="n">review</span><span class="p">):</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">REPLACE_NO_SPACE</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">""</span><span class="p">,</span> <span class="n">review</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">REPLACE_WITH_SPACE</span><span class="o">.</span><span class="n">sub</span><span class="p">(</span><span class="s">" "</span><span class="p">,</span> <span class="n">words</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">words</span>

<span class="k">def</span> <span class="nf">bow_encoding</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">vocabulary</span><span class="p">):</span>
    <span class="n">bow</span> <span class="o">=</span> <span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">vocabulary</span><span class="p">)</span> <span class="c1"># Start by setting the count for each word in the vocabulary to zero.
</span>    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">words</span><span class="o">.</span><span class="n">split</span><span class="p">():</span>  <span class="c1"># For each word in the string
</span>        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">vocabulary</span><span class="p">:</span>  <span class="c1"># If the word is one that occurs in the vocabulary, increase its count.
</span>            <span class="n">bow</span><span class="p">[</span><span class="n">vocabulary</span><span class="p">[</span><span class="n">word</span><span class="p">]]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">return</span> <span class="n">bow</span>


<span class="k">def</span> <span class="nf">lambda_handler</span><span class="p">(</span><span class="n">event</span><span class="p">,</span> <span class="n">context</span><span class="p">):</span>

    <span class="n">vocab</span> <span class="o">=</span> <span class="s">"*** ACTUAL VOCABULARY GOES HERE ***"</span>

    <span class="n">words</span> <span class="o">=</span> <span class="n">review_to_words</span><span class="p">(</span><span class="n">event</span><span class="p">[</span><span class="s">'body'</span><span class="p">])</span>
    <span class="n">bow</span> <span class="o">=</span> <span class="n">bow_encoding</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="n">vocab</span><span class="p">)</span>

    <span class="c1"># The SageMaker runtime is what allows us to invoke the endpoint that we've created.
</span>    <span class="n">runtime</span> <span class="o">=</span> <span class="n">boto3</span><span class="o">.</span><span class="n">Session</span><span class="p">()</span><span class="o">.</span><span class="n">client</span><span class="p">(</span><span class="s">'sagemaker-runtime'</span><span class="p">)</span>

    <span class="c1"># Now we use the SageMaker runtime to invoke our endpoint, sending the review we were given
</span>    <span class="n">response</span> <span class="o">=</span> <span class="n">runtime</span><span class="o">.</span><span class="n">invoke_endpoint</span><span class="p">(</span><span class="n">EndpointName</span> <span class="o">=</span> <span class="s">'***ENDPOINT NAME HERE***'</span><span class="p">,</span><span class="c1"># The name of the endpoint we created
</span>                                       <span class="n">ContentType</span> <span class="o">=</span> <span class="s">'text/csv'</span><span class="p">,</span>                 <span class="c1"># The data format that is expected
</span>                                       <span class="n">Body</span> <span class="o">=</span> <span class="s">','</span><span class="o">.</span><span class="n">join</span><span class="p">([</span><span class="nb">str</span><span class="p">(</span><span class="n">val</span><span class="p">)</span> <span class="k">for</span> <span class="n">val</span> <span class="ow">in</span> <span class="n">bow</span><span class="p">])</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">))</span> <span class="c1"># The actual review
</span>
    <span class="c1"># The response is an HTTP response whose body contains the result of our inference
</span>    <span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s">'Body'</span><span class="p">]</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s">'utf-8'</span><span class="p">)</span>

    <span class="c1"># Round the result so that our web app only gets '1' or '0' as a response.
</span>    <span class="n">result</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">result</span><span class="p">))</span>

    <span class="k">return</span> <span class="p">{</span>
        <span class="s">'statusCode'</span> <span class="p">:</span> <span class="mi">200</span><span class="p">,</span>
        <span class="s">'headers'</span> <span class="p">:</span> <span class="p">{</span> <span class="s">'Content-Type'</span> <span class="p">:</span> <span class="s">'text/plain'</span><span class="p">,</span> <span class="s">'Access-Control-Allow-Origin'</span> <span class="p">:</span> <span class="s">'*'</span> <span class="p">},</span>
        <span class="s">'body'</span> <span class="p">:</span> <span class="nb">str</span><span class="p">(</span><span class="n">result</span><span class="p">)</span>
    <span class="p">}</span>
</code></pre></div></div> <p>We had defined the vocabulary here (in the notebook):</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">train_X</span><span class="p">,</span> <span class="n">test_X</span><span class="p">,</span> <span class="n">vocabulary</span> <span class="o">=</span> <span class="n">extract_BoW_features</span><span class="p">(</span><span class="n">train_X</span><span class="p">,</span> <span class="n">test_X</span><span class="p">)</span>
</code></pre></div></div> <p>We’ll need to replace the <code class="language-plaintext highlighter-rouge">***ENDPOINT NAME HERE***</code> with the results of <code class="language-plaintext highlighter-rouge">xgb_preedictor.endpoint</code> and the <code class="language-plaintext highlighter-rouge">*** ACTUAL VOCABULARY GOES HERE *** with </code>vocabulary`. Click on Save.</p> <p>Create a test event to test it: Dropdown menu → Configure test events → Create new test event → Event template: API Gateaway AWS Proxy → Event name: testEvent</p> <p>Here we can replace <code class="language-plaintext highlighter-rouge">{"body": "test.csv"</code> with <code class="language-plaintext highlighter-rouge">{"body": "This movie is horrible."</code>. Then clikc on test. In order for ou website to use it we need to create an endpoint.</p> <h3 id="building-an-api">Building an API</h3> <p>To create an endpoint for the lambda function go to: Amazon console → Networking &amp; Content Delivery → API Gateaway → Get Started → OK → New API → API name: sentimentAnalysis → Create API</p> <p>The API currently doesn’t do anything, so we’ll need to create some actions: Actions → Create method → POST → click on the checkmark</p> <p>Next we configure our POST action: Integration type: Lambda Function → Check Use Lambda Proxy integration (so it will not do any check on the input data ) → Lambda Function: sentiment_lambda_function &amp;rarr Save</p> <p>Now we also need to deploy the API: Actions → Deploy API → Depoy Stage(environment to deploy to): [New Stage] → Stage Name: prod → Deploy</p> <p>On the top the <code class="language-plaintext highlighter-rouge">URL</code> will appear. If we send a post request to thie URL, the body of the URL will be sent to our lambda function and our lambda function will return the results of our model.</p> <h2 id="launch-the-web-app">Launch the Web App</h2> <p>We are ready to launch our web app. To get the URL of the app, go to: API Gateawa → sentimentAnalysis → Stages and the URL is on the top. Copy it. Going back to the IMDB Sentiment Analysis notebook (in Tutorials), select the <code class="language-plaintext highlighter-rouge">index.html</code> and edit it. Under the form tag, we replace the <code class="language-plaintext highlighter-rouge">action</code> var with our URL. If we save it, the web app is ready. Check the <code class="language-plaintext highlighter-rouge">index.html</code>, download and open the file. The app should be working as expected. Type a few reviews and run the model.</p> <p>Make sure you <strong>SHUT DOWN</strong> the endpoint. Can be doen through the notebook as before, or the console.</p> <p>We also need to delete the Lamba function and the API.</p> <p>To delete the Lambda function go to: Amazon Console → Compute → Lambda → check sentiment_lambda_function → Actions → Delete → Delete</p> <p>To delete the API go to: Amazon Console → Networking and Content Delivery → API Gateaway → sentimentAnalysis → Actions → Delete API → type the name to cofirm deletion</p> <h2 id="update-a-model-with-no-downtime">Update a model with no downtime</h2> <p>We can update our model with no downtine, and we can also feed data to multiple models. We go with the low-level option for this model, so we can have more contol over the model (Boston Housing - Updating an Endpoint).</p> <p>After fitting the model (we haven’t created the model YET), we set up the configuration of the container:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xgb_model_name</span> <span class="o">=</span> <span class="s">"boston-update-xgboost-model"</span> <span class="o">+</span> <span class="n">strftime</span><span class="p">(</span><span class="s">"</span><span class="si">%</span><span class="s">Y-</span><span class="si">%</span><span class="s">m-</span><span class="si">%</span><span class="s">d-</span><span class="si">%</span><span class="s">H-</span><span class="si">%</span><span class="s">M-</span><span class="si">%</span><span class="s">S"</span><span class="p">,</span> <span class="n">gmtime</span><span class="p">())</span>

<span class="c1"># We also need to tell SageMaker which container should be used for inference and where it should
# retrieve the model artifacts from. In our case, the xgboost container that we used for training
# can also be used for inference and the model artifacts come from the previous call to fit.
</span><span class="n">xgb_primary_container</span> <span class="o">=</span> <span class="p">{</span>
    <span class="s">"Image"</span><span class="p">:</span> <span class="n">xgb_container</span><span class="p">,</span>
    <span class="s">"ModelDataUrl"</span><span class="p">:</span> <span class="n">xgb</span><span class="o">.</span><span class="n">model_data</span>
<span class="p">}</span>
</code></pre></div></div> <p>And we create the model:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># And lastly we construct the SageMaker model
</span><span class="n">xgb_model_info</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">sagemaker_client</span><span class="o">.</span><span class="n">create_model</span><span class="p">(</span>
                                <span class="n">ModelName</span> <span class="o">=</span> <span class="n">xgb_model_name</span><span class="p">,</span>
                                <span class="n">ExecutionRoleArn</span> <span class="o">=</span> <span class="n">role</span><span class="p">,</span>
                                <span class="n">PrimaryContainer</span> <span class="o">=</span> <span class="n">xgb_primary_container</span><span class="p">)</span>
</code></pre></div></div> <p>Create the endpoint configuration</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">xgb_endpoint_config_name</span> <span class="o">=</span> <span class="s">"boston-update-xgboost-endpoint-config-"</span> <span class="o">+</span> <span class="n">strftime</span><span class="p">(</span><span class="s">"</span><span class="si">%</span><span class="s">Y-</span><span class="si">%</span><span class="s">m-</span><span class="si">%</span><span class="s">d-</span><span class="si">%</span><span class="s">H-</span><span class="si">%</span><span class="s">M-</span><span class="si">%</span><span class="s">S"</span><span class="p">,</span> <span class="n">gmtime</span><span class="p">())</span>

<span class="c1"># And then we ask SageMaker to construct the endpoint configuration
</span><span class="n">xgb_endpoint_config_info</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">sagemaker_client</span><span class="o">.</span><span class="n">create_endpoint_config</span><span class="p">(</span>
                            <span class="n">EndpointConfigName</span> <span class="o">=</span> <span class="n">xgb_endpoint_config_name</span><span class="p">,</span>
                            <span class="n">ProductionVariants</span> <span class="o">=</span> <span class="p">[{</span>
                                <span class="s">"InstanceType"</span><span class="p">:</span> <span class="s">"ml.m4.xlarge"</span><span class="p">,</span>
                                <span class="s">"InitialVariantWeight"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                                <span class="s">"InitialInstanceCount"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                                <span class="s">"ModelName"</span><span class="p">:</span> <span class="n">xgb_model_name</span><span class="p">,</span>
                                <span class="s">"VariantName"</span><span class="p">:</span> <span class="s">"XGB-Model"</span>
                            <span class="p">}])</span>
</code></pre></div></div> <p>And create and deploy the endpoint:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Again, we need a unique name for our endpoint
</span><span class="n">endpoint_name</span> <span class="o">=</span> <span class="s">"boston-update-endpoint-"</span> <span class="o">+</span> <span class="n">strftime</span><span class="p">(</span><span class="s">"</span><span class="si">%</span><span class="s">Y-</span><span class="si">%</span><span class="s">m-</span><span class="si">%</span><span class="s">d-</span><span class="si">%</span><span class="s">H-</span><span class="si">%</span><span class="s">M-</span><span class="si">%</span><span class="s">S"</span><span class="p">,</span> <span class="n">gmtime</span><span class="p">())</span>

<span class="c1"># And then we can deploy our endpoint
</span><span class="n">endpoint_info</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">sagemaker_client</span><span class="o">.</span><span class="n">create_endpoint</span><span class="p">(</span>
                    <span class="n">EndpointName</span> <span class="o">=</span> <span class="n">endpoint_name</span><span class="p">,</span>
                    <span class="n">EndpointConfigName</span> <span class="o">=</span> <span class="n">xgb_endpoint_config_name</span><span class="p">)</span>
</code></pre></div></div> <p>Finaly, get results back and get the body:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">response</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">sagemaker_runtime_client</span><span class="o">.</span><span class="n">invoke_endpoint</span><span class="p">(</span>
                                                <span class="n">EndpointName</span> <span class="o">=</span> <span class="n">endpoint_name</span><span class="p">,</span>
                                                <span class="n">ContentType</span> <span class="o">=</span> <span class="s">'text/csv'</span><span class="p">,</span>
                                                <span class="n">Body</span> <span class="o">=</span> <span class="s">','</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="nb">str</span><span class="p">,</span> <span class="n">X_test</span><span class="o">.</span><span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">])))</span>
<span class="n">result</span> <span class="o">=</span> <span class="n">response</span><span class="p">[</span><span class="s">'Body'</span><span class="p">]</span><span class="o">.</span><span class="n">read</span><span class="p">()</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s">"utf-8"</span><span class="p">)</span>
</code></pre></div></div> <p>We can make and deploy a second model in the same way we have created the first one, using the exact same endpoint.</p> <h3 id="deply-a-combined-model">Deply a combined model</h3> <p>n order for us to compare our two models, we can perform an <strong>AB test</strong>. We create a new endpoint, and the the endpoint can decide which model to send the data to. we can use both models on its own production variable. The amount of data to be sent to each model, is determined by the initial variant weight:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># As before, we need to give our endpoint configuration a name which should be unique
</span><span class="n">combined_endpoint_config_name</span> <span class="o">=</span> <span class="s">"boston-combined-endpoint-config-"</span> <span class="o">+</span> <span class="n">strftime</span><span class="p">(</span><span class="s">"</span><span class="si">%</span><span class="s">Y-</span><span class="si">%</span><span class="s">m-</span><span class="si">%</span><span class="s">d-</span><span class="si">%</span><span class="s">H-</span><span class="si">%</span><span class="s">M-</span><span class="si">%</span><span class="s">S"</span><span class="p">,</span> <span class="n">gmtime</span><span class="p">())</span>

<span class="c1"># And then we ask SageMaker to construct the endpoint configuration
</span><span class="n">combined_endpoint_config_info</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">sagemaker_client</span><span class="o">.</span><span class="n">create_endpoint_config</span><span class="p">(</span>
                            <span class="n">EndpointConfigName</span> <span class="o">=</span> <span class="n">combined_endpoint_config_name</span><span class="p">,</span>
                            <span class="n">ProductionVariants</span> <span class="o">=</span> <span class="p">[</span>
                                <span class="p">{</span> <span class="c1"># First we include the linear model
</span>                                    <span class="s">"InstanceType"</span><span class="p">:</span> <span class="s">"ml.m4.xlarge"</span><span class="p">,</span>
                                    <span class="s">"InitialVariantWeight"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                                    <span class="s">"InitialInstanceCount"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                                    <span class="s">"ModelName"</span><span class="p">:</span> <span class="n">linear_model_name</span><span class="p">,</span>
                                    <span class="s">"VariantName"</span><span class="p">:</span> <span class="s">"Linear-Model"</span>
                                <span class="p">},</span> <span class="p">{</span> <span class="c1"># And next we include the xgb model
</span>                                    <span class="s">"InstanceType"</span><span class="p">:</span> <span class="s">"ml.m4.xlarge"</span><span class="p">,</span>
                                    <span class="s">"InitialVariantWeight"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                                    <span class="s">"InitialInstanceCount"</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span>
                                    <span class="s">"ModelName"</span><span class="p">:</span> <span class="n">xgb_model_name</span><span class="p">,</span>
                                    <span class="s">"VariantName"</span><span class="p">:</span> <span class="s">"XGB-Model"</span>
                                <span class="p">}])</span>
</code></pre></div></div> <p>We can check the endpoint information by:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">print</span><span class="p">(</span><span class="n">session</span><span class="o">.</span><span class="n">sagemaker_client</span><span class="o">.</span><span class="n">describe_endpoint</span><span class="p">(</span><span class="n">EndpointName</span><span class="o">=</span><span class="n">endpoint_name</span><span class="p">))</span>
</code></pre></div></div> <p>If we want to change the endpoint to only send data to the linear model, we can do that with <code class="language-plaintext highlighter-rouge">sagemaker_client.update_endpoint</code>:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">session</span><span class="o">.</span><span class="n">sagemaker_client</span><span class="o">.</span><span class="n">update_endpoint</span><span class="p">(</span><span class="n">EndpointName</span><span class="o">=</span><span class="n">endpoint_name</span><span class="p">,</span> <span class="n">EndpointConfigName</span><span class="o">=</span><span class="n">linear_endpoint_config_name</span><span class="p">)</span>
</code></pre></div></div> <p>Sagemaker will be reconfigure the endpoint to only have only one production variant, and we will not have any downtime.</p> <h2 id="looking-at-new-data">Looking at new Data</h2> <p>After training and deploying our model, we want to do a quality control of our model, to make sure the accuracy is still in the same levels. To do that we collect a bunch of reviews from our endpoint and label them by hand, and then see what our model says about them.(from mimmi-projects, IMDB Sentiment analysis - XGBoost (Updating a model) notebook) We need to encode them and also to make sure that similar words have the same token. Our vocabulary is the most frequent words, and then we cound the number of each of those words. We need to create our bag of words encoding and then train and create the model as we normally do:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="nn">new_data</span>

<span class="n">new_X</span><span class="p">,</span> <span class="n">new_Y</span> <span class="o">=</span> <span class="n">new_data</span><span class="o">.</span><span class="n">get_new_data</span><span class="p">()</span>
<span class="n">vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">vocabulary</span><span class="o">=</span><span class="n">vocabulary</span><span class="p">,</span>
                <span class="n">preprocessor</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span>

<span class="c1">#Transform our new data set
</span><span class="n">new_XV</span> <span class="o">=</span> <span class="n">vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">new_X</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">new_XV</span><span class="p">)</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s">'new_data.csv'</span><span class="p">),</span> <span class="n">header</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="c1"># upload data
</span><span class="n">new_data_location</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">upload_data</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s">'new_data.csv'</span><span class="p">),</span> <span class="n">key_prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">)</span>

<span class="n">xgb_transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">new_data_location</span><span class="p">,</span> <span class="n">content_type</span><span class="o">=</span><span class="s">'text/csv'</span><span class="p">,</span> <span class="n">split_type</span><span class="o">=</span><span class="s">'Line'</span><span class="p">)</span>
<span class="n">xgb_transformer</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>

</code></pre></div></div> <p>Copy the data to our locar instance</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>!aws s3 cp --recursive $xgb_transformer.output_path $data_dir
</code></pre></div></div> <p>and check the performance on the new data:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">predictions</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s">'new_data.csv.out'</span><span class="p">),</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">num</span><span class="p">)</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">predictions</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">]</span>

<span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_Y</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>

</code></pre></div></div> <p>If we are seeing a lower performance than before, we need to investigate the reviews that have been missclassified.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">sagemaker.predictor</span> <span class="kn">import</span> <span class="n">csv_serializer</span>

<span class="c1"># We need to tell the endpoint what format the data we are sending is in so that SageMaker can perform the serialization.
</span><span class="n">xgb_predictor</span><span class="o">.</span><span class="n">content_type</span> <span class="o">=</span> <span class="s">'text/csv'</span>
<span class="n">xgb_predictor</span><span class="o">.</span><span class="n">serializer</span> <span class="o">=</span> <span class="n">csv_serializer</span>

<span class="c1"># generator
</span><span class="k">def</span> <span class="nf">get_sample</span><span class="p">(</span><span class="n">in_X</span><span class="p">,</span> <span class="n">in_XV</span><span class="p">,</span> <span class="n">in_Y</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">smp</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">in_X</span><span class="p">):</span>
        <span class="n">res</span> <span class="o">=</span> <span class="nb">round</span><span class="p">(</span><span class="nb">float</span><span class="p">(</span><span class="n">xgb_predictor</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">in_XV</span><span class="p">[</span><span class="n">idx</span><span class="p">])))</span>
        <span class="k">if</span> <span class="n">res</span> <span class="o">!=</span> <span class="n">in_Y</span><span class="p">[</span><span class="n">idx</span><span class="p">]:</span>
            <span class="k">yield</span> <span class="n">smp</span><span class="p">,</span> <span class="n">in_Y</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>

<span class="n">gn</span> <span class="o">=</span> <span class="n">get_sample</span><span class="p">(</span><span class="n">new_X</span><span class="p">,</span> <span class="n">new_XV</span><span class="p">,</span> <span class="n">new_Y</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="nb">next</span><span class="p">(</span><span class="n">gn</span><span class="p">))</span>
<span class="n">gn</span> <span class="o">=</span> <span class="n">get_sample</span><span class="p">(</span><span class="n">new_X</span><span class="p">,</span> <span class="n">new_XV</span><span class="p">,</span> <span class="n">new_Y</span><span class="p">)</span>

</code></pre></div></div> <p>Check if the corresponding vocabulary has changed</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_vectorizer</span> <span class="o">=</span> <span class="n">CountVectorizer</span><span class="p">(</span><span class="n">max_features</span><span class="o">=</span><span class="mi">5000</span><span class="p">,</span>
                <span class="n">preprocessor</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">)</span>
<span class="n">new_vectorizer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">new_X</span><span class="p">)</span>
<span class="n">original_vocabulary</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">vocabulary</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
<span class="n">new_vocabulary</span> <span class="o">=</span> <span class="nb">set</span><span class="p">(</span><span class="n">new_vectorizer</span><span class="o">.</span><span class="n">vocabulary_</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>

<span class="k">print</span><span class="p">(</span><span class="n">original_vocabulary</span> <span class="o">-</span> <span class="n">new_vocabulary</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">new_vocabulary</span> <span class="o">-</span> <span class="n">original_vocabulary</span><span class="p">)</span><span class="sb">``</span><span class="err">`</span>
</code></pre></div></div> <p>We might also need to chenge the vocabulary in the lambda functions we created earlier.</p> <p>Encode our dataset:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_XV</span> <span class="o">=</span> <span class="n">new_vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">new_X</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
</code></pre></div></div> <p>And split them</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_val_X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">new_XV</span><span class="p">[:</span><span class="mi">10000</span><span class="p">])</span>
<span class="n">new_train_X</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">new_XV</span><span class="p">[</span><span class="mi">10000</span><span class="p">:])</span>

<span class="n">new_val_y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">new_Y</span><span class="p">[:</span><span class="mi">10000</span><span class="p">])</span>
<span class="n">new_train_y</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">new_Y</span><span class="p">[</span><span class="mi">10000</span><span class="p">:])</span>
</code></pre></div></div> <p>And save them locally:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">new_XV</span><span class="p">)</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s">'new_data.csv'</span><span class="p">),</span> <span class="n">header</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

<span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">new_val_y</span><span class="p">,</span> <span class="n">new_val_X</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s">'new_validation.csv'</span><span class="p">),</span> <span class="n">header</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">pd</span><span class="o">.</span><span class="n">concat</span><span class="p">([</span><span class="n">new_train_y</span><span class="p">,</span> <span class="n">new_train_X</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s">'new_train.csv'</span><span class="p">),</span> <span class="n">header</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
</code></pre></div></div> <p>Make sure you set not nessesary vars to None</p> <p>Now you can crete the model:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_xgb</span> <span class="o">=</span> <span class="n">sagemaker</span><span class="o">.</span><span class="n">estimator</span><span class="o">.</span><span class="n">Estimator</span><span class="p">(</span><span class="n">container</span><span class="p">,</span> <span class="c1"># The location of the container we wish to use
</span>                                    <span class="n">role</span><span class="p">,</span>                                    <span class="c1"># What is our current IAM Role
</span>                                    <span class="n">train_instance_count</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>                  <span class="c1"># How many compute instances
</span>                                    <span class="n">train_instance_type</span><span class="o">=</span><span class="s">'ml.m4.xlarge'</span><span class="p">,</span>      <span class="c1"># What kind of compute instances
</span>                                    <span class="n">output_path</span><span class="o">=</span><span class="s">'s3://{}/{}/output'</span><span class="o">.</span><span class="nb">format</span><span class="p">(</span><span class="n">session</span><span class="o">.</span><span class="n">default_bucket</span><span class="p">(),</span> <span class="n">prefix</span><span class="p">),</span>
                                    <span class="n">sagemaker_session</span><span class="o">=</span><span class="n">session</span><span class="p">)</span>

<span class="c1"># set the algorithm specific parameters
</span><span class="n">new_xgb</span><span class="o">.</span><span class="n">set_hyperparameters</span><span class="p">(</span><span class="n">max_depth</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span>
                        <span class="n">eta</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
                        <span class="n">gamma</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span>
                        <span class="n">min_child_weight</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span>
                        <span class="n">subsample</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span>
                        <span class="n">silent</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
                        <span class="n">objective</span><span class="o">=</span><span class="s">'binary:logistic'</span><span class="p">,</span>
                        <span class="n">early_stopping_rounds</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
                        <span class="n">num_round</span><span class="o">=</span><span class="mi">500</span><span class="p">)</span>

<span class="n">s3_new_input_train</span> <span class="o">=</span> <span class="n">sagemaker</span><span class="o">.</span><span class="n">s3_input</span><span class="p">(</span><span class="n">s3_data</span><span class="o">=</span><span class="n">new_train_location</span><span class="p">,</span> <span class="n">content_type</span><span class="o">=</span><span class="s">'csv'</span><span class="p">)</span>
<span class="n">s3_new_input_validation</span> <span class="o">=</span> <span class="n">sagemaker</span><span class="o">.</span><span class="n">s3_input</span><span class="p">(</span><span class="n">s3_data</span><span class="o">=</span><span class="n">new_val_location</span><span class="p">,</span> <span class="n">content_type</span><span class="o">=</span><span class="s">'csv'</span><span class="p">)</span>

<span class="n">new_xgb</span><span class="o">.</span><span class="n">fit</span><span class="p">({</span><span class="s">'train'</span><span class="p">:</span> <span class="n">s3_new_input_train</span><span class="p">,</span> <span class="s">'validation'</span><span class="p">:</span> <span class="n">s3_new_input_validation</span><span class="p">})</span>
</code></pre></div></div> <p>Finally, test the new model. This is data likeage, as we test in the same it has been trained, but we ONLY do that for comparison with the previous data.</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_xgb_transformer</span> <span class="o">=</span> <span class="n">new_xgb</span><span class="o">.</span><span class="n">transformer</span><span class="p">(</span><span class="n">instance_count</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span> <span class="n">instance_type</span> <span class="o">=</span> <span class="s">'ml.m4.xlarge'</span><span class="p">)</span>
<span class="n">new_xgb_transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">new_data_location</span><span class="p">,</span> <span class="n">content_type</span><span class="o">=</span><span class="s">'text/csv'</span><span class="p">,</span> <span class="n">split_type</span><span class="o">=</span><span class="s">'Line'</span><span class="p">)</span>
<span class="n">new_xgb_transformer</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>

<span class="c1"># Copy the results to our local instance.
</span><span class="err">!</span><span class="n">aws</span> <span class="n">s3</span> <span class="n">cp</span> <span class="o">--</span><span class="n">recursive</span> <span class="err">$</span><span class="n">new_xgb_transformer</span><span class="o">.</span><span class="n">output_path</span> <span class="err">$</span><span class="n">data_dir</span>

<span class="c1"># And see how well the model did.
</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s">'new_data.csv.out'</span><span class="p">),</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">num</span><span class="p">)</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">predictions</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">]</span>

<span class="n">accuracy_score</span><span class="p">(</span><span class="n">new_Y</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span><span class="sb">``</span><span class="err">`</span>


<span class="n">We</span> <span class="n">might</span> <span class="k">as</span> <span class="n">well</span> <span class="n">want</span> <span class="n">to</span> <span class="n">chek</span> <span class="n">whether</span> <span class="n">our</span> <span class="n">model</span> <span class="ow">is</span> <span class="n">behaving</span> <span class="n">ok</span> <span class="n">relatively</span> <span class="n">to</span> <span class="n">the</span> <span class="n">old</span> <span class="n">data</span><span class="p">,</span> <span class="k">as</span> <span class="n">we</span> <span class="n">don</span><span class="s">'t expect the disctribution of the data has changed that much. </span><span class="err">

</span><span class="s">We follow the similar process for the old data:</span><span class="err">

</span><span class="s">```py</span><span class="err">
</span><span class="s">cache_data = None</span><span class="err">
</span><span class="s">with open(os.path.join(cache_dir, "preprocessed_data.pkl"), "rb") as f:</span><span class="err">
</span><span class="s">            cache_data = pickle.load(f)</span><span class="err">
</span><span class="s">            print("Read preprocessed data from cache file:", "preprocessed_data.pkl")</span><span class="err">
</span><span class="s">            </span><span class="err">
</span><span class="s">test_X = cache_data['</span><span class="n">words_test</span><span class="s">']</span><span class="err">
</span><span class="s">test_Y = cache_data['</span><span class="n">labels_test</span><span class="s">']</span><span class="err">

</span><span class="s"># Here we set cache_data to None so that it doesn'</span><span class="n">t</span> <span class="n">occupy</span> <span class="n">memory</span>
<span class="n">cache_data</span> <span class="o">=</span> <span class="bp">None</span>
</code></pre></div></div> <p>Create a bag-of-words encoding and upload them:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">test_X</span> <span class="o">=</span> <span class="n">new_vectorizer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_X</span><span class="p">)</span><span class="o">.</span><span class="n">toarray</span><span class="p">()</span>
<span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">(</span><span class="n">test_X</span><span class="p">)</span><span class="o">.</span><span class="n">to_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s">'test.csv'</span><span class="p">),</span> <span class="n">header</span><span class="o">=</span><span class="bp">False</span><span class="p">,</span> <span class="n">index</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>
<span class="n">test_location</span> <span class="o">=</span> <span class="n">session</span><span class="o">.</span><span class="n">upload_data</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s">'test.csv'</span><span class="p">),</span> <span class="n">key_prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">)</span>
</code></pre></div></div> <p>Test the model again:</p> <div class="language-py highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">new_xgb_transformer</span><span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">test_location</span><span class="p">,</span> <span class="n">content_type</span><span class="o">=</span><span class="s">'text/csv'</span><span class="p">,</span> <span class="n">split_type</span><span class="o">=</span><span class="s">'Line'</span><span class="p">)</span>
<span class="n">new_xgb_transformer</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
<span class="err">!</span><span class="n">aws</span> <span class="n">s3</span> <span class="n">cp</span> <span class="o">--</span><span class="n">recursive</span> <span class="err">$</span><span class="n">new_xgb_transformer</span><span class="o">.</span><span class="n">output_path</span> <span class="err">$</span><span class="n">data_dir</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">read_csv</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">data_dir</span><span class="p">,</span> <span class="s">'test.csv.out'</span><span class="p">),</span> <span class="n">header</span><span class="o">=</span><span class="bp">None</span><span class="p">)</span>
<span class="n">predictions</span> <span class="o">=</span> <span class="p">[</span><span class="nb">round</span><span class="p">(</span><span class="n">num</span><span class="p">)</span> <span class="k">for</span> <span class="n">num</span> <span class="ow">in</span> <span class="n">predictions</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span><span class="o">.</span><span class="n">values</span><span class="p">]</span>
<span class="n">accuracy_score</span><span class="p">(</span><span class="n">test_Y</span><span class="p">,</span> <span class="n">predictions</span><span class="p">)</span>
</code></pre></div></div> <p>If our model is performing well in both old and new data, we can update the model as before. Remember, in order to update en endpoint, we need to crete a new endpoint configuration, and we need to know the name of the mode. Make sure you delete the endpoint at the end. Also make sure you delete any notebook, s3 bucket, lambda or API Gateway.</p> </div> <div id="disqus_thread"></div> </article> <script> var disqus_config = function () { this.page.url = "/ai/2021/02/08/machine_learning_in_production/"; /* Replace PAGE_URL with your page's canonical URL variable */ this.page.identifier = "/ai/2021/02/08/machine_learning_in_production"; /* Replace PAGE_IDENTIFIER with your page's unique identifier variable */ }; (function () { /* DON'T EDIT BELOW THIS LINE */ var d = document, s = d.createElement('script'); s.src = 'https://.disqus.com/embed.js'; s.setAttribute('data-timestamp', +new Date()); (d.head || d.body).appendChild(s); })(); </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a> </noscript></div> </div> <!-- End of row--> <div class="row"> <div class="col-md-4"> <div class="card"> <div class="card-header"> About </div> <div class="card-body"> <!-- Your Bio --> <p class="author_bio"> Hello, My Name is Maria.</p> </div> </div> </div> <div class="col-md-4"> <div class="card"> <div class="card-header">Categories </div> <div class="card-body text-dark"> <div id="#guides"></div> <li class="tag-head"> <a href="/blog/categories/guides">guides</a> </li> <a name="guides"></a> <div id="#dataScience"></div> <li class="tag-head"> <a href="/blog/categories/dataScience">dataScience</a> </li> <a name="dataScience"></a> <div id="#devops"></div> <li class="tag-head"> <a href="/blog/categories/devops">devops</a> </li> <a name="devops"></a> <div id="#resources"></div> <li class="tag-head"> <a href="/blog/categories/resources">resources</a> </li> <a name="resources"></a> <div id="#AI"></div> <li class="tag-head"> <a href="/blog/categories/AI">AI</a> </li> <a name="AI"></a> </div> </div> </div> <div class="col-md-4"> <div class="card"> <div class="card-header">Useful Links </div> <div class="card-body text-dark"> <li > <a href="/about">About Me</a> </li> <li > <a href="/blog">Blog</a> </li> </div> </div> </div> </div> </div> </div> <footer> <p> Powered by<a href="https://github.com/sujaykundu777/devlopr-jekyll"> devlopr jekyll</a>. Hosted at <a href="https://pages.github.com">Github</a>. Subscribe via <a href=" /feed.xml ">RSS</a> </p> </footer> </div> <script> var options = { classname: 'my-class', id: 'my-id' }; var nanobar = new Nanobar( options ); nanobar.go( 30 ); nanobar.go( 76 ); nanobar.go(100); </script> <div hidden id="snipcart" data-api-key="Y2I1NTAyNWYtMTNkMy00ODg0LWE4NDItNTZhYzUxNzJkZTI5NjM3MDI4NTUzNzYyMjQ4NzU0"></div> <script src="https://cdn.snipcart.com/themes/v3.0.0-beta.3/default/snipcart.js" defer></script> <script src="/assets/js/mode-switcher.js"></script> </body> </html>
